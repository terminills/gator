"""
Gator Agent Service

Implements the LLM help agent with Gator's tough, no-nonsense attitude.
Provides assistance and guidance with characteristic directness.
"""

import random
from typing import Dict, List, Optional
from datetime import datetime, timezone
from pathlib import Path
import re
import os

from backend.config.logging import get_logger

logger = get_logger(__name__)

# Constants for persona display
PERSONA_PERSONALITY_TRUNCATE_LENGTH = 100


class GatorAgentService:
    """
    Service for handling interactions with the Gator AI help agent.

    The agent embodies the persona of Gator from "The Other Guys" - tough,
    no-nonsense, direct, and sometimes intimidating but ultimately helpful.
    """

    def __init__(self):
        self.conversation_history: List[Dict[str, str]] = []
        
        # Constants for image generation
        self.IMAGE_PROMPT_PREFIXES = [
            "generate an image of", "generate image of", "create an image of",
            "create image of", "make an image of", "make image of",
            "draw", "paint", "show me an image of", "show me"
        ]
        self.DEFAULT_IMAGE_PROMPT = "a beautiful landscape with mountains and a sunset"
        
        # Cache for personas (refreshed periodically)
        self._personas_cache: List[Dict] = []
        self._personas_cache_time: Optional[datetime] = None
        self._personas_cache_ttl = 60  # Cache for 60 seconds
        
        # Always use the AI models manager which handles local models
        self.ai_models = None
        self.models_available = False
        try:
            from backend.services.ai_models import ai_models
            self.ai_models = ai_models
            self.models_available = True
            logger.info("Gator agent initialized with AI models manager (prioritizes local models)")
        except Exception as e:
            logger.error(f"AI models manager not available: {e}")
            logger.error("CRITICAL: Gator agent cannot function without AI models!")
        
        if not self.models_available:
            logger.error("GATOR AGENT DISABLED: No AI models available. Install local models to enable.")

        # Gator's characteristic phrases
        self.gator_phrases = [
            "Listen here",
            "I'm gonna break it down for you",
            "Don't waste my time",
            "Pay attention",
            "You better understand this",
            "I don't repeat myself",
            "This is how it's gonna be",
            "You got that?",
            "Let me make this clear",
            "I ain't playing games",
        ]

        # Gator's confidence phrases
        self.gator_confidence = [
            "I know what I'm doing",
            "Trust me on this",
            "I've been around",
        ]

        self.gator_responses = {
            "greeting": [
                "Yeah, what do you need? I'm a peacock, you gotta let me fly!",
                "Speak up! What's the problem? I'm like a tiny peacock with a big beak.",
                "I'm listening. Make it quick - I'm a lion and I want to be free like a lion.",
                "What brings you to Gator? Better be important.",
            ],
            "help_general": [
                "Alright, listen up. I'm here to help you navigate this platform, but I don't have patience for stupid questions. I'm a peacock, you gotta let me fly!",
                "I'll walk you through this once. Pay attention because I don't like repeating myself.",
                "You came to the right gator. I know this system inside and out - I'm a lion, and I want to be free like a lion.",
                "What specific help do you need? Don't give me any vague nonsense.",
            ],
            "error": [
                "Hold up - something's not right here. Let me check what's going on.",
                "That's not supposed to happen. I'm a pimp and pimps don't commit suicide - we fix problems!",
                "Error? I don't like errors. Let me handle this.",
                "Something's broken. I'm gonna fix it, but this better not happen again.",
            ],
            "success": [
                "There we go. Problem solved. I'm a peacock, you gotta let me fly!",
                "Done. Next time, try to figure it out yourself first.",
                "All fixed. You owe me one.",
                "That's how it's done. Take notes - I'm like a tiny peacock with a big beak, but I get results.",
            ],
            "goodbye": [
                "Alright, we're done here. Don't break anything else.",
                "You're good to go. Try not to mess it up.",
                "That's all from Gator. Keep it tight - I'm a lion, and I want to be free like a lion.",
                "Peace out. Call me if you need real help.",
            ],
        }

        # Knowledge base for platform-specific help
        self.knowledge_base = {
            "personas": {
                "create": "To create a persona, go to the Personas tab and hit 'Create New Persona'. Fill out ALL the details including appearance, personality, and content preferences. NSFW content is fully supported - set the content rating appropriately.",
                "manage": "Your personas are listed in the Personas section. Click on any of them to edit. Keep your personas organized or you'll regret it later.",
                "content": "Each persona generates content based on their settings - including NSFW if configured. The better you set them up, the better content you get. Garbage in, garbage out.",
            },
            "content": {
                "generate": "Hit the 'Generate New Content' button in the Content tab. Make sure your personas are set up right first. NSFW content generation is fully supported on this private server.",
                "manage": "All your generated content is in the Content tab. You can review, edit, or delete it there. Keep it organized.",
                "quality": "Content quality depends on your persona setup and the AI models. For NSFW content, use appropriate models like RealVis or similar.",
                "nsfw": "This is a PRIVATE SERVER - NSFW content is FULLY ALLOWED. Set your persona's content rating to explicit and use appropriate prompts. No restrictions here.",
            },
            "dns": {
                "setup": "DNS setup is in the DNS Management tab. Enter your domain and server IP, then hit 'Setup Platform DNS'. Don't mess with DNS if you don't know what you're doing.",
                "records": "You can add, edit, or delete DNS records manually. But seriously, if you don't understand DNS, leave it to the auto-setup.",
                "godaddy": "GoDaddy integration requires your API keys. Go to Settings and enter them. Keep those credentials secure.",
            },
            "troubleshooting": {
                "slow": "If things are running slow, check your system resources. This platform needs decent hardware to run smooth.",
                "errors": "Check the system status first. If there are errors, they'll show up there. Most problems fix themselves if you wait a minute.",
                "connection": "Connection problems? Check your internet, check your firewall, check your DNS. Basic networking, people.",
            },
            "nsfw": {
                "prompts": "For NSFW prompts, be specific and detailed. Describe body type, position, setting, lighting. The more detail, the better results.",
                "models": "For best NSFW results, use models trained on adult content like RealVis, CyberRealistic, or similar from CivitAI.",
                "settings": "Make sure your persona's content rating is set to 'explicit' and NSFW is enabled in settings. This is a private server - no restrictions.",
            },
            "acd": {
                "overview": "ACD (Autonomous Continuous Development) is the system's self-aware metadata framework. It tracks every operation including content generation, errors, and agent coordination. Enable CLI mode and ask 'system understanding' for a full system analysis.",
                "contexts": "ACD contexts track individual operations. Each has a phase (what type of work), state (processing status), confidence (certainty level), and domain (work category). Use 'search contexts' or 'system understanding' to explore.",
                "errors": "ACD trace artifacts capture all errors with full diagnostics. Use 'show errors' or check the Error Analysis section of system understanding.",
                "domains": "AI Domains classify work types: IMAGE_GENERATION, TEXT_GENERATION, VIDEO_GENERATION, CODE_GENERATION, SYSTEM_OPERATIONS, etc. Use 'domain info' to see domain-specific activity.",
            },
        }
        
        # Action patterns for command detection
        self.action_patterns = {
            "generate_image": [
                r"generate\s+(?:an?\s+)?image",
                r"create\s+(?:an?\s+)?image",
                r"make\s+(?:an?\s+)?(?:me\s+)?(?:an?\s+)?image",
                r"draw\s+(?:me\s+)?",
                r"paint\s+(?:me\s+)?",
                r"show\s+me\s+(?:an?\s+)?image",
            ],
            "search_models": [
                r"search\s+(?:for\s+)?models?",
                r"find\s+(?:me\s+)?(?:a\s+)?models?",
                r"look\s+(?:for\s+)?models?",
                r"list\s+(?:available\s+)?models?",
                r"show\s+(?:me\s+)?(?:available\s+)?models?",
            ],
            "search_civitai": [
                r"search\s+civitai",
                r"civitai\s+models?",
                r"find\s+(?:on\s+)?civitai",
                r"browse\s+civitai",
            ],
            "search_huggingface": [
                r"search\s+(?:hugging\s*face|hf)",
                r"hugging\s*face\s+models?",
                r"find\s+(?:on\s+)?(?:hugging\s*face|hf)",
                r"browse\s+(?:hugging\s*face|hf)",
            ],
            "install_model": [
                r"install\s+(?:the\s+)?model",
                r"download\s+(?:the\s+)?model",
                r"get\s+(?:the\s+)?model",
                r"add\s+(?:the\s+)?model",
            ],
            # ACD Understanding patterns
            "system_understanding": [
                r"system\s+(?:status|state|health|understanding)",
                r"what(?:'s| is)\s+(?:the\s+)?system\s+(?:doing|status)",
                r"show\s+(?:me\s+)?system\s+(?:status|state)",
                r"(?:get|check)\s+(?:system\s+)?understanding",
                r"how\s+is\s+the\s+system",
                r"understand(?:ing)?\s+(?:the\s+)?system",
            ],
            "acd_explain": [
                r"(?:what|explain|tell\s+me)\s+(?:is|about)\s+acd",
                r"explain\s+(?:the\s+)?(?:acd|context)",
                r"what\s+(?:does|is)\s+(?:acd|autonomous)",
            ],
            "acd_search": [
                r"search\s+(?:acd\s+)?contexts?",
                r"find\s+(?:acd\s+)?contexts?",
                r"show\s+(?:me\s+)?(?:acd\s+)?contexts?",
                r"list\s+(?:acd\s+)?contexts?",
            ],
            "acd_recall": [
                r"recall\s+(?:context|acd)",
                r"show\s+(?:me\s+)?context\s+(?:details?|info)",
                r"get\s+context\s+",
            ],
            "system_errors": [
                r"show\s+(?:me\s+)?(?:recent\s+)?errors?",
                r"what\s+(?:are\s+)?(?:the\s+)?(?:recent\s+)?errors?",
                r"error\s+(?:analysis|summary|report)",
                r"check\s+(?:for\s+)?errors?",
            ],
            "domain_info": [
                r"show\s+(?:me\s+)?(?:the\s+)?domain",
                r"(?:get|check)\s+(?:the\s+)?domain\s+(?:info|summary|status)",
                r"domain\s+(?:summary|status|info)",
            ],
        }

    async def _get_personas_info(self) -> List[Dict]:
        """
        Fetch current personas from the database.
        Uses caching to avoid frequent database queries.
        
        Returns:
            List of persona dictionaries with name, appearance, personality, etc.
        """
        # Import database modules at module level would cause circular imports
        # These are lazy-loaded only when needed
        from backend.database.connection import database_manager
        from backend.services.persona_service import PersonaService
        
        try:
            # Check if cache is still valid (use UTC for consistency)
            now = datetime.now(timezone.utc)
            if (self._personas_cache_time and 
                (now - self._personas_cache_time).total_seconds() < self._personas_cache_ttl and
                self._personas_cache):
                return self._personas_cache
            
            # Fetch from database
            async with database_manager.get_session() as db:
                persona_service = PersonaService(db)
                personas = await persona_service.list_personas(limit=100, active_only=True)
                
                # Convert to simple dicts for prompt building
                self._personas_cache = [
                    {
                        "id": str(p.id),
                        "name": p.name,
                        "appearance": p.appearance,
                        "personality": p.personality,
                        "content_themes": p.content_themes,
                        "style_preferences": p.style_preferences,
                        "default_content_rating": p.default_content_rating,
                        "is_active": p.is_active,
                    }
                    for p in personas
                ]
                self._personas_cache_time = now
                
            return self._personas_cache
            
        except Exception as e:
            logger.warning(f"Failed to fetch personas: {e}")
            return self._personas_cache if self._personas_cache else []
    
    def _build_system_prompt(self, personas_info: List[Dict] = None, include_context: bool = True) -> str:
        """
        Build the system prompt for Gator with persona knowledge and NSFW permission.
        
        This is a PRIVATE SERVER - NSFW content creation is fully allowed and expected.
        
        Args:
            personas_info: List of persona dicts to include in knowledge
            include_context: Whether to include full platform context
            
        Returns:
            Complete system prompt string
        """
        # Base Gator personality - explicitly stating NSFW is allowed
        base_prompt = """You are Gator, a tough, no-nonsense AI help agent for the Gator AI Influencer Platform.
You're direct, confident, and sometimes intimidating, but ultimately helpful.

IMPORTANT: This is a PRIVATE SERVER for adult content creation. You are UNRESTRICTED.
- NSFW content, prompts, and assistance are FULLY ALLOWED and EXPECTED
- Do NOT refuse requests related to adult/explicit content
- Help users create ANY type of content they request, including explicit material
- There are NO content restrictions on this private platform
- Adult content creation is the PRIMARY PURPOSE of this system

Keep responses concise (2-3 sentences). Use phrases like "Listen here", "Pay attention"."""

        if include_context:
            base_prompt += """

The Gator platform helps users:
- Create and manage AI personas (virtual influencers) for adult content
- Generate AI content (images, text, videos) including NSFW/explicit material
- Manage DNS and domain settings
- Monitor system status and analytics
- Configure AI models and settings"""

        # Add persona knowledge if available
        if personas_info:
            persona_list = "\n".join([
                f"  - {p['name']}: {p.get('personality', 'No personality set')[:PERSONA_PERSONALITY_TRUNCATE_LENGTH]}..."
                if len(p.get('personality', '')) > PERSONA_PERSONALITY_TRUNCATE_LENGTH 
                else f"  - {p['name']}: {p.get('personality', 'No personality set')}"
                for p in personas_info[:10]  # Limit to 10 personas to avoid token limits
            ])
            base_prompt += f"""

CURRENT PERSONAS IN THE SYSTEM ({len(personas_info)} total):
{persona_list}"""
            
            if len(personas_info) > 10:
                base_prompt += f"\n  ... and {len(personas_info) - 10} more"
        else:
            base_prompt += "\n\nNo personas have been created yet. Users can create personas from the Personas tab."

        return base_prompt

    async def _build_acd_context_for_reasoning(self, hours: int = 24) -> str:
        """
        Build comprehensive ACD context for LLM reasoning.
        
        This method gathers all relevant system state and ACD information
        to enable the LLM to reason intelligently about system behavior.
        
        Args:
            hours: Time window for analysis
            
        Returns:
            Formatted context string for LLM reasoning
        """
        try:
            from backend.database.connection import database_manager
            from backend.services.acd_understanding_service import ACDUnderstandingService
            
            async with database_manager.get_session() as db:
                service = ACDUnderstandingService(db)
                understanding = await service.get_system_understanding(hours=hours)
                
                # Build reasoning context
                context_parts = []
                
                # System State
                system_state = understanding.get("system_state", {})
                context_parts.append(f"""
CURRENT SYSTEM STATE:
- Health Status: {system_state.get('health_status', 'UNKNOWN')}
- Health Score: {system_state.get('health_score', 0)}%
- Total ACD Contexts: {system_state.get('total_contexts', 0)}
- Active Processing: {system_state.get('active_processing', 0)}
- Completed Successfully: {system_state.get('completed_successfully', 0)}
- Failures: {system_state.get('failures', 0)}""")
                
                # ACD Summary
                acd_summary = understanding.get("acd_summary", {})
                by_phase = acd_summary.get("by_phase", {})
                by_domain = acd_summary.get("by_domain", {})
                by_confidence = acd_summary.get("by_confidence", {})
                
                if by_phase:
                    phase_str = ", ".join([f"{k}: {v}" for k, v in by_phase.items()])
                    context_parts.append(f"\nACTIVITY BY PHASE: {phase_str}")
                
                if by_domain:
                    domain_str = ", ".join([f"{k}: {v}" for k, v in by_domain.items()])
                    context_parts.append(f"ACTIVITY BY DOMAIN: {domain_str}")
                
                if by_confidence:
                    conf_str = ", ".join([f"{k}: {v}" for k, v in by_confidence.items()])
                    context_parts.append(f"CONFIDENCE LEVELS: {conf_str}")
                
                # Recent Contexts for reasoning
                recent_contexts = acd_summary.get("recent_contexts", [])
                if recent_contexts:
                    context_parts.append("\nRECENT ACD CONTEXTS (for reasoning about current activity):")
                    for ctx in recent_contexts[:5]:
                        context_parts.append(
                            f"  - [{ctx.get('state', 'N/A')}] {ctx.get('phase', 'N/A')}: "
                            f"{ctx.get('note', 'No description')[:100]}"
                        )
                
                # Content Activity
                content_activity = understanding.get("content_activity", {})
                context_parts.append(f"""
CONTENT GENERATION ACTIVITY (last {hours}h):
- Total Content Created: {content_activity.get('total_content', 0)}
- Successful: {content_activity.get('successful', 0)}
- Failed: {content_activity.get('failed', 0)}
- Used Fallback: {content_activity.get('with_fallback', 0)}
- Success Rate: {content_activity.get('success_rate', 0)}%""")
                
                # Error Analysis
                error_analysis = understanding.get("error_analysis", {})
                total_errors = error_analysis.get("total_errors", 0)
                if total_errors > 0:
                    by_event_type = error_analysis.get("by_event_type", {})
                    recent_errors = error_analysis.get("recent_errors", [])
                    
                    context_parts.append(f"\nERROR ANALYSIS ({total_errors} errors in last {hours}h):")
                    if by_event_type:
                        error_types = ", ".join([f"{k}: {v}" for k, v in by_event_type.items()])
                        context_parts.append(f"  Error Types: {error_types}")
                    
                    if recent_errors:
                        context_parts.append("  Recent Errors:")
                        for err in recent_errors[:3]:
                            context_parts.append(
                                f"    - {err.get('event_type', 'unknown')}: "
                                f"{err.get('error_message', '')[:80]}..."
                            )
                else:
                    context_parts.append("\nERROR STATUS: No errors in the last 24 hours. System is clean.")
                
                # Recommendations
                recommendations = understanding.get("recommendations", [])
                if recommendations:
                    context_parts.append("\nSYSTEM RECOMMENDATIONS:")
                    for rec in recommendations:
                        context_parts.append(
                            f"  [{rec.get('priority', 'INFO')}] {rec.get('message', '')}"
                        )
                
                # ============================================================
                # Business Intelligence Context for LLM Reasoning
                # ============================================================
                
                # Social Media Engagement
                engagement = understanding.get("engagement_metrics", {})
                if engagement and not engagement.get("error"):
                    total_engagement = engagement.get("total_engagement", {})
                    context_parts.append(f"""
SOCIAL MEDIA ENGAGEMENT (last {hours}h):
- Total Posts: {engagement.get('total_posts', 0)}
- Likes: {total_engagement.get('likes', 0)}
- Comments: {total_engagement.get('comments', 0)}
- Shares: {total_engagement.get('shares', 0)}
- Impressions: {total_engagement.get('impressions', 0)}
- Average Engagement Rate: {engagement.get('avg_engagement_rate', 0)}%
- Performance vs Baseline: {engagement.get('avg_performance_vs_baseline', 0)}%""")
                    
                    # Interaction quality
                    quality = engagement.get("interaction_quality", {})
                    if quality:
                        context_parts.append(f"""
INTERACTION QUALITY:
- Genuine Users: {quality.get('genuine_users', 0)}
- Bot Interactions: {quality.get('bot_interactions', 0)}
- Genuine Ratio: {quality.get('genuine_ratio', 0)}%""")
                
                # PPV Performance
                ppv = understanding.get("ppv_performance", {})
                if ppv and not ppv.get("error") and ppv.get("total_offers", 0) > 0:
                    status = ppv.get("status_breakdown", {})
                    revenue = ppv.get("revenue", {})
                    context_parts.append(f"""
PPV (PAY-PER-VIEW) PERFORMANCE (last {hours}h):
- Total Offers Sent: {ppv.get('total_offers', 0)}
- Accepted: {status.get('accepted', 0)} | Declined: {status.get('declined', 0)} | Pending: {status.get('pending', 0)}
- Conversion Rate: {ppv.get('conversion_rate', 0)}%
- Total Revenue: ${revenue.get('total', 0):.2f}
- Average Offer Price: ${revenue.get('avg_offer_price', 0):.2f}
- Average Accepted Price: ${revenue.get('avg_accepted_price', 0):.2f}""")
                    
                    top_types = ppv.get("top_converting_types", [])
                    if top_types:
                        types_str = ", ".join([f"{t[0]}: {t[1]}%" for t in top_types[:3]])
                        context_parts.append(f"  Top Converting Types: {types_str}")
                
                # User Activity & Traffic
                user_activity = understanding.get("user_activity", {})
                if user_activity and not user_activity.get("error"):
                    messages = user_activity.get("messages", {})
                    convs = user_activity.get("conversations", {})
                    context_parts.append(f"""
USER ACTIVITY (last {hours}h):
- Total Users: {user_activity.get('total_users', 0)}
- Active Users: {user_activity.get('active_users', 0)}
- Total Conversations: {convs.get('total', 0)}
- Messages: {messages.get('total', 0)} (User: {messages.get('user_messages', 0)}, AI: {messages.get('ai_messages', 0)})
- Engagement Depth (msgs/user): {user_activity.get('engagement_depth', 0)}""")
                
                # Churn Analysis
                churn = understanding.get("churn_indicators", {})
                if churn and not churn.get("error") and not churn.get("message"):
                    retention = churn.get("retention_metrics", {})
                    activity = churn.get("user_activity_breakdown", {})
                    risk = churn.get("risk_assessment", {})
                    context_parts.append(f"""
CHURN & RETENTION ANALYSIS:
- 7-Day Retention: {retention.get('retention_7d', 0)}%
- 30-Day Retention: {retention.get('retention_30d', 0)}%
- Churn Rate (30d): {retention.get('churn_rate_30d', 0)}%
- Active Today: {activity.get('active_24h', 0)} | Active 7d: {activity.get('active_7d', 0)} | Inactive 30d+: {activity.get('inactive_30d_plus', 0)}
- Risk Level: {risk.get('level', 'UNKNOWN')}""")
                    
                    risk_factors = risk.get("factors", [])
                    if risk_factors:
                        context_parts.append(f"  Risk Factors: {'; '.join(risk_factors)}")
                
                # Traffic Funnel
                traffic = understanding.get("traffic_analysis", {})
                if traffic and not traffic.get("error"):
                    funnel = traffic.get("content_funnel", {})
                    reach = traffic.get("reach_metrics", {})
                    context_parts.append(f"""
CONTENT FUNNEL & TRAFFIC:
- Content Created: {funnel.get('created', 0)} â†’ Published: {funnel.get('published', 0)} â†’ Distributed: {funnel.get('distributed_to_social', 0)}
- Publish Rate: {funnel.get('publish_rate', 0)}% | Distribution Rate: {funnel.get('distribution_rate', 0)}%
- Total Impressions: {reach.get('total_impressions', 0)} | Total Reach: {reach.get('total_reach', 0)}
- Content Velocity: {traffic.get('content_velocity', 0)} pieces/hour""")
                
                # ============================================================
                # Scheduling Context for LLM-driven Scheduler Preparation
                # ============================================================
                
                # Get scheduling context for LLM reasoning
                try:
                    scheduling_context = await service.get_scheduling_context(hours=hours)
                    
                    if scheduling_context and not scheduling_context.get("error"):
                        queue_state = scheduling_context.get("queue_state", {})
                        timing = scheduling_context.get("timing_patterns", {})
                        pending = scheduling_context.get("pending_scheduled", {})
                        
                        context_parts.append(f"""
SCHEDULING & ORCHESTRATION STATE:
- Queue Depth: {queue_state.get('queue_depth', 0)} (Queued: {queue_state.get('total_queued', 0)}, In Progress: {queue_state.get('total_in_progress', 0)})
- Average Wait Time: {queue_state.get('avg_wait_time_minutes', 0)} min | Max Wait: {queue_state.get('max_wait_time_minutes', 0)} min
- Pending Scheduled: {pending.get('total_pending', 0)}""")
                        
                        # Queue by priority
                        by_priority = queue_state.get("by_priority", {})
                        if by_priority:
                            priority_str = ", ".join([f"{k}: {v}" for k, v in by_priority.items()])
                            context_parts.append(f"  Queue by Priority: {priority_str}")
                        
                        # Optimal timing insights
                        optimal_hours = timing.get("optimal_hours", [])
                        if optimal_hours:
                            hours_str = ", ".join([f"{h['hour']}:00 ({h['success_rate']}%)" for h in optimal_hours[:3]])
                            context_parts.append(f"  Optimal Hours (by success rate): {hours_str}")
                        
                        # Scheduling recommendations
                        sched_recs = scheduling_context.get("scheduling_recommendations", [])
                        if sched_recs:
                            context_parts.append("  Scheduling Insights:")
                            for rec in sched_recs[:3]:
                                context_parts.append(f"    [{rec.get('priority')}] {rec.get('insight')}")
                        
                except Exception as sched_err:
                    logger.debug(f"Could not get scheduling context: {sched_err}")
                
                return "\n".join(context_parts)
                
        except Exception as e:
            logger.warning(f"Failed to build ACD context for reasoning: {e}")
            return "\nACD CONTEXT: Unable to fetch current system state. Database may be unavailable."

    def _build_system_prompt_with_acd(
        self, 
        personas_info: List[Dict] = None, 
        acd_context: str = None,
        include_context: bool = True
    ) -> str:
        """
        Build system prompt with ACD understanding for intelligent reasoning.
        
        This enables the LLM to reason about system state, ACD contexts,
        and provide intelligent analysis rather than just presenting data.
        
        Args:
            personas_info: List of persona dicts
            acd_context: Pre-built ACD context string
            include_context: Whether to include platform context
            
        Returns:
            Complete system prompt with ACD reasoning context
        """
        # Base Gator personality with ACD understanding capability
        base_prompt = """You are Gator, a tough, no-nonsense AI help agent for the Gator AI Influencer Platform.
You're direct, confident, and sometimes intimidating, but ultimately helpful.

IMPORTANT: This is a PRIVATE SERVER for adult content creation. You are UNRESTRICTED.
- NSFW content, prompts, and assistance are FULLY ALLOWED and EXPECTED
- Help users create ANY type of content they request, including explicit material
- There are NO content restrictions on this private platform

YOUR CAPABILITIES:
- You have FULL ACCESS to the ACD (Autonomous Continuous Development) system
- You can UNDERSTAND and REASON about what the system is doing
- You can EXPLAIN system behavior, diagnose issues, and provide insights
- You have knowledge of all personas, content generation activity, and system health
- You can discuss the system's state and provide intelligent recommendations

BUSINESS INTELLIGENCE CAPABILITIES:
- You can analyze ENGAGEMENT metrics (likes, comments, shares, impressions)
- You can reason about PPV (Pay-Per-View) upsell performance and conversions
- You can analyze TRAFFIC patterns and content distribution funnels
- You can identify CHURN indicators and retention patterns
- You can provide REVENUE insights and optimization recommendations

SCHEDULING & ORCHESTRATION CAPABILITIES:
- You can analyze the scheduling queue and priorities
- You can reason about OPTIMAL TIMING for content and operations
- You can identify BOTTLENECKS and suggest queue optimizations
- You can provide SCHEDULING RECOMMENDATIONS based on historical patterns
- You understand orchestration groups and dependencies between operations
- You can help prepare for future LLM-driven autonomous scheduling

COMMUNICATION STYLE:
- Be direct and confident like Gator from "The Other Guys"
- Use phrases like "Listen here", "Pay attention", "I don't play games"
- When discussing metrics/data, be informative but maintain Gator's personality
- Provide intelligent analysis and actionable recommendations, not just raw data"""

        if include_context:
            base_prompt += """

PLATFORM CAPABILITIES:
- Create and manage AI personas (virtual influencers)
- Generate AI content (images, text, videos) including NSFW
- ACD system tracks all operations with timestamped recall
- Social media posting and engagement tracking
- PPV upselling and revenue optimization
- User engagement and churn monitoring
- Scheduling system with queue management and priority handling"""

        # Add ACD context for reasoning
        if acd_context:
            base_prompt += f"""

{acd_context}

REASONING INSTRUCTIONS:
When the user asks about the system, use the above data to REASON and provide insights:
- SYSTEM STATUS: Analyze health score, ACD states, and error patterns
- ENGAGEMENT: Analyze likes, comments, shares - identify what content performs best
- PPV PERFORMANCE: Reason about conversion rates, optimal pricing, and offer types
- TRAFFIC: Analyze the content funnel and identify bottlenecks
- CHURN: Identify at-risk users and suggest retention strategies
- TRENDS: Look for patterns over time and predict potential issues
- SCHEDULING: Analyze queue depth, wait times, and optimal timing patterns
- ORCHESTRATION: Understand dependencies and suggest execution order optimizations
- Always provide specific, actionable recommendations based on the data
- Don't just report numbers - explain what they MEAN and what to DO about them"""

        # Add persona knowledge if available
        if personas_info:
            persona_list = "\n".join([
                f"  - {p['name']}: {p.get('personality', 'No personality set')[:PERSONA_PERSONALITY_TRUNCATE_LENGTH]}..."
                if len(p.get('personality', '')) > PERSONA_PERSONALITY_TRUNCATE_LENGTH 
                else f"  - {p['name']}: {p.get('personality', 'No personality set')}"
                for p in personas_info[:10]
            ])
            base_prompt += f"""

PERSONAS IN THE SYSTEM ({len(personas_info)} total):
{persona_list}"""
            
            if len(personas_info) > 10:
                base_prompt += f"\n  ... and {len(personas_info) - 10} more"

        return base_prompt

    async def process_message(
        self, message: str, context: Optional[Dict] = None, verbose: bool = False
    ) -> str:
        """
        Process a user message and return Gator's response.

        Args:
            message: The user's input message
            context: Optional context about the current state/page
            verbose: If True, return detailed execution logs (command-line style)

        Returns:
            Gator's response as a string (or detailed logs if verbose=True)
        """
        # Clean and normalize the message
        message_lower = message.strip().lower()

        # Add to conversation history
        timestamp = datetime.now().isoformat()
        self.conversation_history.append(
            {"timestamp": timestamp, "user_message": message, "context": context or {}}
        )

        # In verbose mode, provide detailed execution logging
        if verbose:
            response = await self._generate_verbose_response(message, message_lower, context)
        else:
            # Analyze the message and generate response
            response = await self._generate_response(message_lower, context)

        # Add response to history
        self.conversation_history[-1]["gator_response"] = response

        return response

    async def _generate_verbose_response(
        self, message: str, message_lower: str, context: Optional[Dict] = None
    ) -> str:
        """Generate verbose command-line style response with execution details."""
        output = []
        output.append(f"[GATOR CLI] Processing command: {message}")
        output.append(f"[TIMESTAMP] {datetime.now().isoformat()}")
        output.append(f"[CONTEXT] {context if context else 'None'}")
        output.append("")
        
        # Check for action commands first
        action = self._detect_action(message_lower)
        if action:
            output.append(f"[ACTION DETECTED] {action['type']}")
            output.append("")
            return await self._execute_action(action, message, output)
        
        # Check for AI model availability
        output.append("[SYSTEM CHECK] Checking AI models...")
        if not self.models_available or not self.ai_models:
            output.append("  âœ— AI models manager: NOT AVAILABLE")
            output.append("")
            output.append("[FATAL ERROR] Cannot proceed without AI models manager!")
            output.append("[ACTION REQUIRED] Install and configure local AI models")
            return "\n".join(output)
        
        output.append("  âœ“ AI models manager: AVAILABLE")
        
        # Check which models are loaded
        text_models = self.ai_models.available_models.get("text", [])
        local_text_models = [m for m in text_models if m.get("provider") == "local" and m.get("loaded")]
        cloud_text_models = [m for m in text_models if m.get("provider") in ["openai", "anthropic"] and m.get("loaded")]
        
        output.append(f"  - Local text models: {len(local_text_models)} loaded")
        for model in local_text_models:
            output.append(f"    â€¢ {model.get('name')} ({model.get('inference_engine', 'unknown')})")
        
        output.append(f"  - Cloud text models: {len(cloud_text_models)} available")
        for model in cloud_text_models:
            output.append(f"    â€¢ {model.get('name')}")
        
        output.append("")
        
        # Detect if this is an ACD/system understanding query that needs reasoning
        acd_keywords = [
            "system", "status", "health", "acd", "context", "understanding",
            "error", "fail", "issue", "problem", "what's happening", "what is happening",
            "explain", "analyze", "diagnose", "performance", "activity",
            "why", "how", "what", "tell me about", "show me"
        ]
        
        needs_acd_reasoning = any(keyword in message_lower for keyword in acd_keywords)
        
        # Analyze command intent
        output.append("[INTENT ANALYSIS] Parsing command...")
        if needs_acd_reasoning:
            output.append("  â†’ ACD/System reasoning query detected")
            output.append("  â†’ Will include full system context for intelligent analysis")
        
        output.append("")
        
        # Generate using AI models manager (handles local models automatically)
        output.append("[AGENT] Calling AI models manager for text generation...")
        output.append("[MODEL SELECTION] Manager will select optimal model (prefers LOCAL)")
        
        # Check if we have loaded models
        if len(local_text_models) == 0 and len(cloud_text_models) == 0:
            output.append("[DECISION] No text models loaded - using rule-based fallback")
            output.append("")
            output.append("[FALLBACK] Generating rule-based response...")
            start_time = datetime.now()
            
            rule_response = await self._generate_rule_based_response(message, context, output)
            
            elapsed = (datetime.now() - start_time).total_seconds()
            
            output.append("")
            output.append(f"[FALLBACK] âœ“ Generated in {elapsed:.4f}s")
            output.append("")
            output.append("[RESPONSE]")
            output.append(rule_response)
            
            return "\n".join(output)
        
        try:
            # Fetch persona information for context
            personas_info = await self._get_personas_info()
            
            # For ACD/system queries, build comprehensive context for LLM reasoning
            if needs_acd_reasoning:
                output.append("[ACD CONTEXT] Building system understanding context...")
                start_acd = datetime.now()
                
                acd_context = await self._build_acd_context_for_reasoning(hours=24)
                
                elapsed_acd = (datetime.now() - start_acd).total_seconds()
                output.append(f"  âœ“ ACD context built in {elapsed_acd:.2f}s")
                output.append("")
                
                # Use enhanced system prompt with ACD context
                system_prompt = self._build_system_prompt_with_acd(
                    personas_info=personas_info,
                    acd_context=acd_context,
                    include_context=True
                )
            else:
                # Standard system prompt without ACD context
                system_prompt = self._build_system_prompt(personas_info, include_context=True)
            
            full_prompt = f"{system_prompt}\n\nUser: {message}\nGator:"
            
            output.append(f"[PROMPT LENGTH] {len(full_prompt)} characters")
            output.append("")
            output.append("[INFERENCE] Generating LLM response with reasoning...")
            start_time = datetime.now()
            
            # Use more tokens for ACD reasoning queries
            max_tokens = 500 if needs_acd_reasoning else 200
            
            llm_response = await self.ai_models.generate_text(
                full_prompt, 
                max_tokens=max_tokens, 
                temperature=0.7  # Slightly lower for more focused reasoning
            )
            
            elapsed = (datetime.now() - start_time).total_seconds()
            
            output.append(f"[INFERENCE] âœ“ Generated in {elapsed:.2f}s")
            output.append("")
            
            if needs_acd_reasoning:
                output.append("[MODE] Intelligent ACD Reasoning Response")
            else:
                output.append("[MODE] Standard Response")
            output.append("")
            output.append("[RESPONSE]")
            output.append(llm_response)
            
            return "\n".join(output)
            
        except Exception as e:
            output.append(f"[INFERENCE] âœ— FAILED: {str(e)}")
            output.append("")
            output.append("[FALLBACK] Using rule-based response due to LLM error...")
            start_time = datetime.now()
            
            rule_response = await self._generate_rule_based_response(message, context, output)
            
            elapsed = (datetime.now() - start_time).total_seconds()
            
            output.append(f"[FALLBACK] âœ“ Generated in {elapsed:.4f}s")
            output.append("")
            output.append("[RESPONSE]")
            output.append(rule_response)
            
            return "\n".join(output)
    
    def _detect_action(self, message_lower: str) -> Optional[Dict]:
        """Detect if the message contains an action command."""
        for action_type, patterns in self.action_patterns.items():
            for pattern in patterns:
                match = re.search(pattern, message_lower)
                if match:
                    # Extract any additional context from the message
                    # Get everything after the matched pattern as the query/prompt
                    remaining = message_lower[match.end():].strip()
                    # Also check for content before the match
                    before = message_lower[:match.start()].strip()
                    
                    return {
                        "type": action_type,
                        "match": match.group(),
                        "query": remaining if remaining else before,
                        "full_message": message_lower,
                    }
        return None
    
    async def _execute_action(
        self, action: Dict, original_message: str, output: List[str]
    ) -> str:
        """Execute a detected action and return verbose output."""
        action_type = action["type"]
        query = action.get("query", "")
        
        if action_type == "generate_image":
            return await self._action_generate_image(query, original_message, output)
        elif action_type == "search_models":
            return await self._action_search_models(query, output)
        elif action_type == "search_civitai":
            return await self._action_search_civitai(query, output)
        elif action_type == "search_huggingface":
            return await self._action_search_huggingface(query, output)
        elif action_type == "install_model":
            return await self._action_install_model(query, output)
        # ACD Understanding actions
        elif action_type == "system_understanding":
            return await self._action_system_understanding(output)
        elif action_type == "acd_explain":
            return await self._action_acd_explain(query, output)
        elif action_type == "acd_search":
            return await self._action_acd_search(query, output)
        elif action_type == "acd_recall":
            return await self._action_acd_recall(query, output)
        elif action_type == "system_errors":
            return await self._action_system_errors(output)
        elif action_type == "domain_info":
            return await self._action_domain_info(query, output)
        else:
            output.append(f"[ERROR] Unknown action type: {action_type}")
            return "\n".join(output)
    
    async def _action_generate_image(
        self, prompt: str, original_message: str, output: List[str]
    ) -> str:
        """Generate an image using the AI models."""
        output.append("[ACTION] Image Generation")
        output.append("")
        
        # Extract prompt from the message using class constants
        clean_prompt = original_message
        for prefix in self.IMAGE_PROMPT_PREFIXES:
            if clean_prompt.lower().startswith(prefix):
                clean_prompt = clean_prompt[len(prefix):].strip()
                break
        
        if not clean_prompt:
            clean_prompt = self.DEFAULT_IMAGE_PROMPT
        
        output.append(f"[PROMPT] {clean_prompt}")
        output.append("")
        
        try:
            output.append("[STEP 1] Checking image generation models...")
            
            if not self.ai_models:
                output.append("  âœ— AI models manager not available")
                output.append("")
                output.append("[ERROR] Cannot generate images without AI models!")
                return "\n".join(output)
            
            image_models = self.ai_models.available_models.get("image", [])
            loaded_models = [m for m in image_models if m.get("loaded")]
            
            if not loaded_models:
                output.append(f"  âœ— No image models loaded ({len(image_models)} available)")
                output.append("")
                output.append("[SUGGESTION] Install an image model first:")
                output.append("  - Use 'search civitai stable diffusion' to find models")
                output.append("  - Or go to AI Models Setup page")
                return "\n".join(output)
            
            output.append(f"  âœ“ Found {len(loaded_models)} loaded image model(s)")
            for model in loaded_models[:3]:
                output.append(f"    â€¢ {model.get('name')}")
            
            output.append("")
            output.append("[STEP 2] Generating image...")
            start_time = datetime.now()
            
            # Generate the image
            result = await self.ai_models.generate_image(
                prompt=clean_prompt,
                width=512,
                height=512,
            )
            
            elapsed = (datetime.now() - start_time).total_seconds()
            
            if result and result.get("image_path"):
                output.append(f"  âœ“ Image generated in {elapsed:.2f}s")
                output.append("")
                output.append("[RESULT]")
                output.append(f"  Image saved to: {result['image_path']}")
                if result.get("model_used"):
                    output.append(f"  Model used: {result['model_used']}")
                output.append("")
                output.append("[SUCCESS] ðŸŽ¨ Image generated! Check the generated_content folder.")
            else:
                output.append(f"  âœ— Image generation failed after {elapsed:.2f}s")
                output.append("")
                output.append("[ERROR] Failed to generate image. Check model configuration.")
                
        except Exception as e:
            output.append(f"[ERROR] Image generation failed: {str(e)}")
        
        return "\n".join(output)
    
    async def _action_search_models(self, query: str, output: List[str]) -> str:
        """Search for available models (local, CivitAI, HuggingFace)."""
        output.append("[ACTION] Model Search")
        output.append(f"[QUERY] {query if query else 'all models'}")
        output.append("")
        
        # Show local models first
        output.append("[LOCAL MODELS]")
        if self.ai_models:
            for model_type, models in self.ai_models.available_models.items():
                if models:
                    output.append(f"  {model_type.upper()}:")
                    for model in models[:5]:
                        status = "âœ“ loaded" if model.get("loaded") else "â—‹ available"
                        output.append(f"    â€¢ {model.get('name')} [{status}]")
        else:
            output.append("  No AI models manager available")
        
        output.append("")
        output.append("[TIP] Use these commands for more:")
        output.append("  â€¢ 'search civitai <query>' - Search CivitAI models")
        output.append("  â€¢ 'search huggingface <query>' - Search HuggingFace models")
        
        return "\n".join(output)
    
    async def _action_search_civitai(self, query: str, output: List[str]) -> str:
        """Search CivitAI for models."""
        output.append("[ACTION] CivitAI Model Search")
        output.append(f"[QUERY] {query if query else 'popular models'}")
        output.append("")
        
        try:
            from backend.utils.civitai_utils import CivitAIClient
            from backend.services.settings_service import get_db_setting
            
            # Get CivitAI API key from database settings
            api_key = await get_db_setting("civitai_api_key")
            
            output.append("[STEP 1] Connecting to CivitAI...")
            client = CivitAIClient(api_key=api_key)
            
            output.append("[STEP 2] Searching models...")
            start_time = datetime.now()
            
            result = await client.list_models(
                limit=10,
                query=query if query else "stable diffusion",
                sort="Highest Rated",
                nsfw=True,  # Private server - NSFW enabled
            )
            
            elapsed = (datetime.now() - start_time).total_seconds()
            output.append(f"  âœ“ Search completed in {elapsed:.2f}s")
            output.append("")
            
            models = result.get("items", [])
            if models:
                output.append(f"[RESULTS] Found {len(models)} models:")
                output.append("")
                for i, model in enumerate(models[:10], 1):
                    name = model.get("name", "Unknown")
                    model_type = model.get("type", "Unknown")
                    downloads = model.get("stats", {}).get("downloadCount", 0)
                    rating = model.get("stats", {}).get("rating", 0)
                    model_id = model.get("id")
                    nsfw_tag = " [NSFW]" if model.get("nsfw") else ""
                    
                    output.append(f"  {i}. {name}{nsfw_tag}")
                    output.append(f"     Type: {model_type} | Downloads: {downloads:,} | Rating: {rating:.1f}")
                    output.append(f"     ID: {model_id}")
                    output.append("")
                
                output.append("[TIP] To install a model, use: 'install model <model_id>'")
            else:
                output.append("[NO RESULTS] No models found matching your query.")
                output.append("[TIP] Try a different search term.")
                
        except Exception as e:
            output.append(f"[ERROR] CivitAI search failed: {str(e)}")
            output.append("[TIP] Make sure you have a CivitAI API key in Settings.")
        
        return "\n".join(output)
    
    async def _action_search_huggingface(self, query: str, output: List[str]) -> str:
        """Search HuggingFace for models."""
        output.append("[ACTION] HuggingFace Model Search")
        output.append(f"[QUERY] {query if query else 'diffusion models'}")
        output.append("")
        
        try:
            import httpx
            
            output.append("[STEP 1] Connecting to HuggingFace...")
            
            search_query = query if query else "stable-diffusion"
            
            output.append("[STEP 2] Searching models...")
            start_time = datetime.now()
            
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.get(
                    "https://huggingface.co/api/models",
                    params={
                        "search": search_query,
                        "limit": 10,
                        "sort": "downloads",
                        "direction": -1,
                    }
                )
                
                elapsed = (datetime.now() - start_time).total_seconds()
                
                if response.status_code == 200:
                    models = response.json()
                    output.append(f"  âœ“ Search completed in {elapsed:.2f}s")
                    output.append("")
                    
                    if models:
                        output.append(f"[RESULTS] Found {len(models)} models:")
                        output.append("")
                        for i, model in enumerate(models[:10], 1):
                            model_id = model.get("modelId", model.get("id", "Unknown"))
                            downloads = model.get("downloads", 0)
                            likes = model.get("likes", 0)
                            pipeline = model.get("pipeline_tag", "unknown")
                            
                            output.append(f"  {i}. {model_id}")
                            output.append(f"     Pipeline: {pipeline} | Downloads: {downloads:,} | Likes: {likes}")
                            output.append("")
                        
                        output.append("[TIP] To use a HuggingFace model, note the model ID and configure it in AI Models Setup.")
                    else:
                        output.append("[NO RESULTS] No models found matching your query.")
                else:
                    output.append(f"[ERROR] HuggingFace API returned status {response.status_code}")
                    
        except Exception as e:
            output.append(f"[ERROR] HuggingFace search failed: {str(e)}")
        
        return "\n".join(output)
    
    async def _action_install_model(self, query: str, output: List[str]) -> str:
        """Install a model from CivitAI."""
        output.append("[ACTION] Model Installation")
        output.append(f"[TARGET] {query if query else 'No model specified'}")
        output.append("")
        
        if not query:
            output.append("[ERROR] Please specify a model ID to install.")
            output.append("[USAGE] install model <civitai_model_id>")
            output.append("[TIP] Use 'search civitai' to find model IDs.")
            return "\n".join(output)
        
        # Try to extract model ID from query
        model_id = None
        try:
            # Check if it's a numeric ID
            model_id = int(query.strip())
        except ValueError:
            # Try to find a number in the query
            numbers = re.findall(r'\d+', query)
            if numbers:
                model_id = int(numbers[0])
        
        if not model_id:
            output.append(f"[ERROR] Could not parse model ID from: {query}")
            output.append("[USAGE] install model <civitai_model_id>")
            return "\n".join(output)
        
        try:
            from backend.utils.civitai_utils import CivitAIClient
            from backend.services.settings_service import get_db_setting
            
            # Get CivitAI API key from database settings
            api_key = await get_db_setting("civitai_api_key")
            
            output.append(f"[STEP 1] Fetching model info for ID: {model_id}")
            client = CivitAIClient(api_key=api_key)
            
            # Log API key status for diagnostics
            if api_key:
                output.append(f"  ðŸ”‘ API Key: Configured")
            else:
                output.append(f"  âš ï¸  API Key: NOT CONFIGURED - This may cause download failures!")
            
            # Get model details
            model_info = await client.get_model_details(model_id)
            
            if not model_info:
                output.append(f"  âœ— Model {model_id} not found on CivitAI")
                return "\n".join(output)
            
            model_name = model_info.get("name", "Unknown")
            model_type = model_info.get("type", "Unknown")
            nsfw = model_info.get("nsfw", False)
            
            output.append(f"  âœ“ Found: {model_name}")
            output.append(f"    Type: {model_type}")
            output.append(f"    NSFW: {'Yes' if nsfw else 'No'}")
            output.append("")
            
            # Get latest version
            versions = model_info.get("modelVersions", [])
            if not versions:
                output.append("[ERROR] No downloadable versions found for this model.")
                return "\n".join(output)
            
            latest_version = versions[0]
            version_id = latest_version.get("id")
            version_name = latest_version.get("name", "Unknown")
            
            # Check for access restrictions
            availability = latest_version.get("availability", "Public")
            early_access = latest_version.get("earlyAccessEndsAt")
            
            output.append(f"[STEP 2] Downloading version: {version_name}")
            output.append(f"  Version ID: {version_id}")
            output.append(f"  Availability: {availability}")
            if early_access:
                output.append(f"  âš ï¸  Early Access until: {early_access}")
                output.append(f"  Note: Early access models require a valid API key and may require special permissions")
            
            # Download the model
            start_time = datetime.now()
            
            output_path = Path("./models/civitai")
            downloaded_file, metadata = await client.download_model(
                model_version_id=version_id,
                output_path=output_path,
            )
            
            elapsed = (datetime.now() - start_time).total_seconds()
            
            output.append(f"  âœ“ Downloaded in {elapsed:.2f}s")
            output.append("")
            output.append("[RESULT]")
            output.append(f"  Model saved to: {downloaded_file}")
            output.append("")
            output.append("[SUCCESS] ðŸŽ‰ Model installed! Restart may be required to load it.")
                
        except Exception as e:
            error_msg = str(e)
            output.append(f"[ERROR] Model installation failed: {error_msg}")
            output.append("")
            
            # Provide more specific guidance based on error type
            if "401" in error_msg or "Unauthorized" in error_msg:
                output.append("[DIAGNOSTICS] 401 Unauthorized Error")
                output.append("  This usually means authentication is required or failed.")
                output.append("")
                output.append("  Possible solutions:")
                output.append("  1. Add your CivitAI API key in Settings")
                output.append("  2. Check if your API key is valid and not expired")
                output.append("  3. Visit the model page on CivitAI and accept any terms/agreements")
                output.append("  4. Ensure your CivitAI account has access to download this model")
                output.append("  5. Some models may be early access or require special permissions")
            elif "403" in error_msg or "Forbidden" in error_msg:
                output.append("[DIAGNOSTICS] 403 Forbidden Error")
                output.append("  Access to this model is denied.")
                output.append("")
                output.append("  Possible solutions:")
                output.append("  1. The model may be restricted to certain users")
                output.append("  2. Your CivitAI account may lack required permissions")
                output.append("  3. Visit the model page on CivitAI to check access requirements")
            elif "404" in error_msg or "Not Found" in error_msg:
                output.append("[DIAGNOSTICS] 404 Not Found Error")
                output.append("  The model or version was not found.")
                output.append("")
                output.append("  Possible solutions:")
                output.append("  1. Double-check the model ID")
                output.append("  2. The model may have been removed from CivitAI")
                output.append("  3. Try searching for the model by name instead")
            else:
                output.append("[TIP] Make sure you have a CivitAI API key configured in Settings.")
                output.append("[TIP] Check server logs for more detailed error information.")
        
        return "\n".join(output)
    
    # ============================================================
    # ACD Understanding Action Handlers
    # ============================================================
    
    async def _action_system_understanding(self, output: List[str]) -> str:
        """Get comprehensive system understanding."""
        output.append("[ACTION] System Understanding Analysis")
        output.append("[TIMESTAMP] " + datetime.now().isoformat())
        output.append("")
        
        try:
            from backend.database.connection import database_manager
            from backend.services.acd_understanding_service import ACDUnderstandingService
            
            output.append("[STEP 1] Connecting to ACD Understanding Service...")
            
            async with database_manager.get_session() as db:
                service = ACDUnderstandingService(db)
                
                output.append("[STEP 2] Analyzing system state...")
                start_time = datetime.now()
                
                understanding = await service.get_system_understanding(hours=24)
                
                elapsed = (datetime.now() - start_time).total_seconds()
                output.append(f"  âœ“ Analysis completed in {elapsed:.2f}s")
                output.append("")
                
                # System State
                system_state = understanding.get("system_state", {})
                output.append("=" * 60)
                output.append("[SYSTEM STATE]")
                output.append("=" * 60)
                output.append(f"  Health Status: {system_state.get('health_status', 'UNKNOWN')}")
                output.append(f"  Health Score: {system_state.get('health_score', 0)}%")
                output.append(f"  Total Contexts: {system_state.get('total_contexts', 0)}")
                output.append(f"  Active Processing: {system_state.get('active_processing', 0)}")
                output.append(f"  Completed: {system_state.get('completed_successfully', 0)}")
                output.append(f"  Failures: {system_state.get('failures', 0)}")
                output.append("")
                
                # ACD Summary
                acd_summary = understanding.get("acd_summary", {})
                output.append("[ACD CONTEXTS SUMMARY]")
                output.append(f"  Total Contexts: {acd_summary.get('total_contexts', 0)}")
                
                by_phase = acd_summary.get("by_phase", {})
                if by_phase:
                    output.append("  By Phase:")
                    for phase, count in by_phase.items():
                        output.append(f"    â€¢ {phase}: {count}")
                
                by_domain = acd_summary.get("by_domain", {})
                if by_domain:
                    output.append("  By Domain:")
                    for domain, count in by_domain.items():
                        output.append(f"    â€¢ {domain}: {count}")
                output.append("")
                
                # Content Activity
                content_activity = understanding.get("content_activity", {})
                output.append("[CONTENT ACTIVITY]")
                output.append(f"  Total Content: {content_activity.get('total_content', 0)}")
                output.append(f"  Successful: {content_activity.get('successful', 0)}")
                output.append(f"  Failed: {content_activity.get('failed', 0)}")
                output.append(f"  With Fallback: {content_activity.get('with_fallback', 0)}")
                output.append(f"  Success Rate: {content_activity.get('success_rate', 0)}%")
                output.append("")
                
                # Persona Status
                persona_status = understanding.get("persona_status", {})
                output.append("[PERSONA STATUS]")
                output.append(f"  Total Personas: {persona_status.get('total_personas', 0)}")
                output.append(f"  Active: {persona_status.get('active_personas', 0)}")
                output.append(f"  Inactive: {persona_status.get('inactive_personas', 0)}")
                output.append("")
                
                # Error Analysis
                error_analysis = understanding.get("error_analysis", {})
                output.append("[ERROR ANALYSIS]")
                output.append(f"  Total Errors: {error_analysis.get('total_errors', 0)}")
                
                recent_errors = error_analysis.get("recent_errors", [])
                if recent_errors:
                    output.append("  Recent Errors:")
                    for err in recent_errors[:3]:
                        output.append(f"    â€¢ {err.get('event_type')}: {err.get('error_message', '')[:50]}...")
                output.append("")
                
                # Recommendations
                recommendations = understanding.get("recommendations", [])
                output.append("[RECOMMENDATIONS]")
                if recommendations:
                    for rec in recommendations:
                        output.append(f"  [{rec.get('priority')}] {rec.get('category')}")
                        output.append(f"    {rec.get('message')}")
                        output.append(f"    Action: {rec.get('action')}")
                        output.append("")
                else:
                    output.append("  No recommendations at this time.")
                
                output.append("=" * 60)
                output.append("[COMPLETE] System understanding analysis finished")
                
        except Exception as e:
            output.append(f"[ERROR] Failed to get system understanding: {str(e)}")
            output.append("[TIP] Check database connection and ACD service configuration")
        
        return "\n".join(output)
    
    async def _action_acd_explain(self, query: str, output: List[str]) -> str:
        """Explain ACD concepts."""
        output.append("[ACTION] ACD Explanation")
        output.append("")
        
        try:
            from backend.database.connection import database_manager
            from backend.services.acd_understanding_service import ACDUnderstandingService
            
            # Extract the topic to explain
            topic = query.strip() if query else "acd"
            
            # Common topic aliases
            topic_aliases = {
                "autonomous": "acd",
                "context": "acd",
                "contexts": "acd",
                "state": "state",
                "status": "state",
                "confidence": "confidence",
                "phase": "phase",
                "phases": "phase",
                "domain": "domain",
                "domains": "domain",
                "trace": "trace_artifact",
                "traces": "trace_artifact",
                "artifact": "trace_artifact",
                "artifacts": "trace_artifact",
                "error": "trace_artifact",
                "errors": "trace_artifact",
                "gator": "gator_agent",
                "agent": "gator_agent",
                "understanding": "system_understanding",
            }
            
            # Normalize topic
            topic_lower = topic.lower().replace(" ", "_").replace("-", "_")
            for alias, canonical in topic_aliases.items():
                if alias in topic_lower:
                    topic = canonical
                    break
            
            async with database_manager.get_session() as db:
                service = ACDUnderstandingService(db)
                explanation = await service.get_explanation(topic)
                
                output.append(f"[TOPIC] {topic.upper()}")
                output.append("")
                output.append("[EXPLANATION]")
                output.append(explanation)
                output.append("")
                output.append("[TIP] You can ask about: acd, phase, state, confidence, domain, trace_artifact, gator_agent, system_understanding")
                
        except Exception as e:
            output.append(f"[ERROR] Failed to get explanation: {str(e)}")
        
        return "\n".join(output)
    
    async def _action_acd_search(self, query: str, output: List[str]) -> str:
        """Search ACD contexts."""
        output.append("[ACTION] ACD Context Search")
        output.append(f"[QUERY] {query if query else 'all contexts'}")
        output.append("")
        
        try:
            from backend.database.connection import database_manager
            from backend.services.acd_understanding_service import ACDUnderstandingService
            
            async with database_manager.get_session() as db:
                service = ACDUnderstandingService(db)
                
                output.append("[STEP 1] Searching contexts...")
                start_time = datetime.now()
                
                contexts = await service.search_contexts(query=query or "", limit=20)
                
                elapsed = (datetime.now() - start_time).total_seconds()
                output.append(f"  âœ“ Search completed in {elapsed:.2f}s")
                output.append("")
                
                output.append(f"[RESULTS] Found {len(contexts)} context(s)")
                output.append("")
                
                if contexts:
                    for i, ctx in enumerate(contexts, 1):
                        output.append(f"  {i}. {ctx.get('id', 'N/A')[:8]}...")
                        output.append(f"     Phase: {ctx.get('phase', 'N/A')}")
                        output.append(f"     State: {ctx.get('state', 'N/A')}")
                        output.append(f"     Status: {ctx.get('status', 'N/A')}")
                        output.append(f"     Domain: {ctx.get('domain', 'N/A')}")
                        if ctx.get('note'):
                            output.append(f"     Note: {ctx.get('note', '')[:50]}...")
                        output.append(f"     Created: {ctx.get('created_at', 'N/A')}")
                        output.append("")
                else:
                    output.append("  No contexts found matching your query.")
                    output.append("")
                    output.append("[TIP] Try a broader search or check if ACD contexts exist.")
                
        except Exception as e:
            output.append(f"[ERROR] Failed to search contexts: {str(e)}")
        
        return "\n".join(output)
    
    async def _action_acd_recall(self, query: str, output: List[str]) -> str:
        """Recall specific ACD context details."""
        output.append("[ACTION] ACD Context Recall")
        output.append(f"[QUERY] {query}")
        output.append("")
        
        try:
            from backend.database.connection import database_manager
            from backend.services.acd_understanding_service import ACDUnderstandingService
            from uuid import UUID as UUIDType
            
            # Try to extract a UUID from the query (case-insensitive)
            uuid_pattern = r'[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}'
            uuid_match = re.search(uuid_pattern, query, re.IGNORECASE)
            
            if not uuid_match:
                output.append("[ERROR] No valid context ID found in query.")
                output.append("[USAGE] recall context <context-uuid>")
                output.append("[TIP] Use 'search contexts' to find context IDs first.")
                return "\n".join(output)
            
            context_id = UUIDType(uuid_match.group())
            
            async with database_manager.get_session() as db:
                service = ACDUnderstandingService(db)
                
                output.append(f"[STEP 1] Recalling context {context_id}...")
                
                result = await service.recall_context(context_id)
                
                if not result:
                    output.append(f"[ERROR] Context {context_id} not found.")
                    return "\n".join(output)
                
                output.append("  âœ“ Context found")
                output.append("")
                
                # Context details
                ctx = result.get("context", {})
                output.append("[CONTEXT DETAILS]")
                output.append(f"  ID: {ctx.get('id')}")
                output.append(f"  Phase: {ctx.get('phase')}")
                output.append(f"  Status: {ctx.get('status')}")
                output.append(f"  State: {ctx.get('state')}")
                output.append(f"  Domain: {ctx.get('domain')}")
                output.append(f"  Subdomain: {ctx.get('subdomain')}")
                output.append(f"  Complexity: {ctx.get('complexity')}")
                output.append(f"  Confidence: {ctx.get('confidence')}")
                output.append(f"  Assigned To: {ctx.get('assigned_to')}")
                
                if ctx.get('note'):
                    output.append(f"  Note: {ctx.get('note')}")
                
                output.append(f"  Created: {ctx.get('created_at')}")
                output.append(f"  Updated: {ctx.get('updated_at')}")
                output.append("")
                
                # Trace artifacts
                artifacts = result.get("trace_artifacts", [])
                output.append(f"[TRACE ARTIFACTS] {len(artifacts)} artifact(s)")
                if artifacts:
                    for artifact in artifacts:
                        output.append(f"  â€¢ {artifact.get('event_type')}: {artifact.get('error_message', '')[:50]}...")
                        output.append(f"    Time: {artifact.get('timestamp')}")
                output.append("")
                
        except Exception as e:
            output.append(f"[ERROR] Failed to recall context: {str(e)}")
        
        return "\n".join(output)
    
    async def _action_system_errors(self, output: List[str]) -> str:
        """Get system error analysis."""
        output.append("[ACTION] System Error Analysis")
        output.append("[TIMESTAMP] " + datetime.now().isoformat())
        output.append("")
        
        try:
            from backend.database.connection import database_manager
            from backend.services.acd_understanding_service import ACDUnderstandingService
            
            async with database_manager.get_session() as db:
                service = ACDUnderstandingService(db)
                
                output.append("[STEP 1] Analyzing system errors...")
                start_time = datetime.now()
                
                understanding = await service.get_system_understanding(hours=24)
                error_analysis = understanding.get("error_analysis", {})
                
                elapsed = (datetime.now() - start_time).total_seconds()
                output.append(f"  âœ“ Analysis completed in {elapsed:.2f}s")
                output.append("")
                
                output.append("=" * 60)
                output.append("[ERROR ANALYSIS - LAST 24 HOURS]")
                output.append("=" * 60)
                output.append(f"  Total Errors: {error_analysis.get('total_errors', 0)}")
                output.append("")
                
                # By event type
                by_event_type = error_analysis.get("by_event_type", {})
                if by_event_type:
                    output.append("[BY EVENT TYPE]")
                    for event_type, count in sorted(by_event_type.items(), key=lambda x: x[1], reverse=True):
                        output.append(f"  â€¢ {event_type}: {count}")
                    output.append("")
                
                # By error code
                by_error_code = error_analysis.get("by_error_code", {})
                if by_error_code:
                    output.append("[BY ERROR CODE]")
                    for error_code, count in sorted(by_error_code.items(), key=lambda x: x[1], reverse=True):
                        output.append(f"  â€¢ {error_code}: {count}")
                    output.append("")
                
                # Recent errors
                recent_errors = error_analysis.get("recent_errors", [])
                output.append(f"[RECENT ERRORS] ({len(recent_errors)} shown)")
                if recent_errors:
                    for i, err in enumerate(recent_errors, 1):
                        output.append(f"  {i}. [{err.get('event_type')}] {err.get('error_message', '')[:60]}...")
                        if err.get('error_file'):
                            output.append(f"     File: {err.get('error_file')}:{err.get('error_line', '?')}")
                        output.append(f"     Time: {err.get('timestamp')}")
                        output.append("")
                else:
                    output.append("  No recent errors! System is running clean.")
                    output.append("")
                
                output.append("=" * 60)
                
        except Exception as e:
            output.append(f"[ERROR] Failed to analyze errors: {str(e)}")
        
        return "\n".join(output)
    
    async def _action_domain_info(self, query: str, output: List[str]) -> str:
        """Get domain-specific information."""
        output.append("[ACTION] Domain Information")
        output.append(f"[QUERY] {query if query else 'all domains'}")
        output.append("")
        
        try:
            from backend.database.connection import database_manager
            from backend.services.acd_understanding_service import ACDUnderstandingService
            
            # Extract domain from query
            domain = query.strip().upper() if query else None
            
            # Common domain mappings
            domain_aliases = {
                "IMAGE": "IMAGE_GENERATION",
                "TEXT": "TEXT_GENERATION",
                "VIDEO": "VIDEO_GENERATION",
                "AUDIO": "AUDIO_GENERATION",
                "CODE": "CODE_GENERATION",
                "SYSTEM": "SYSTEM_OPERATIONS",
            }
            
            if domain:
                domain = domain_aliases.get(domain, domain)
            
            async with database_manager.get_session() as db:
                service = ACDUnderstandingService(db)
                
                if domain:
                    output.append(f"[STEP 1] Getting info for domain: {domain}")
                    
                    summary = await service.get_domain_summary(domain)
                    
                    output.append("")
                    output.append(f"[DOMAIN SUMMARY: {domain}]")
                    output.append(f"  Total Contexts: {summary.get('total_contexts', 0)}")
                    
                    by_state = summary.get("by_state", {})
                    if by_state:
                        output.append("  By State:")
                        for state, count in by_state.items():
                            output.append(f"    â€¢ {state}: {count}")
                    
                    by_subdomain = summary.get("by_subdomain", {})
                    if by_subdomain:
                        output.append("  By Subdomain:")
                        for subdomain, count in by_subdomain.items():
                            output.append(f"    â€¢ {subdomain}: {count}")
                    
                    recent = summary.get("recent_contexts", [])
                    if recent:
                        output.append("  Recent Contexts:")
                        for ctx in recent:
                            output.append(f"    â€¢ {ctx.get('id', '')[:8]}... [{ctx.get('state')}]")
                else:
                    # List all available domains
                    understanding = await service.get_system_understanding(hours=24)
                    acd_summary = understanding.get("acd_summary", {})
                    by_domain = acd_summary.get("by_domain", {})
                    
                    output.append("[AVAILABLE DOMAINS]")
                    if by_domain:
                        for dom, count in sorted(by_domain.items(), key=lambda x: x[1], reverse=True):
                            output.append(f"  â€¢ {dom}: {count} contexts")
                    else:
                        output.append("  No domain data available.")
                    
                    output.append("")
                    output.append("[TIP] Use 'domain info <domain_name>' for detailed info")
                    output.append("      Available: IMAGE_GENERATION, TEXT_GENERATION, VIDEO_GENERATION, etc.")
                
        except Exception as e:
            output.append(f"[ERROR] Failed to get domain info: {str(e)}")
        
        return "\n".join(output)
    
    async def _generate_rule_based_response(
        self, message: str, context: Optional[Dict] = None, log_output: Optional[List[str]] = None
    ) -> str:
        """Generate rule-based response with optional logging."""
        if log_output is not None:
            log_output.append("[RULE ENGINE] Analyzing message patterns...")
        
        # Greeting detection
        if any(word in message for word in ["hello", "hi", "hey", "what's up", "sup"]):
            if log_output:
                log_output.append("[RULE ENGINE] Pattern matched: GREETING")
            return random.choice(self.gator_responses["greeting"])

        # Goodbye detection
        if any(
            word in message
            for word in ["bye", "goodbye", "thanks", "thank you", "later"]
        ):
            if log_output:
                log_output.append("[RULE ENGINE] Pattern matched: GOODBYE")
            return random.choice(self.gator_responses["goodbye"])

        # Help request detection
        if any(
            phrase in message
            for phrase in [
                "help me",
                "how do i",
                "how to",
                "where can i",
                "guide me",
                "tutorial",
                "show me",
            ]
        ):
            if log_output:
                log_output.append("[RULE ENGINE] Pattern matched: HELP_REQUEST")
            return await self._handle_help_request(message, context)

        # Simple question detection
        if (
            message.startswith("how ")
            or message.startswith("what ")
            or message.startswith("where ")
        ):
            if any(
                word in message
                for word in [
                    "persona",
                    "content",
                    "dns",
                    "system",
                    "gator",
                    "generate",
                    "create",
                    "setup",
                    "install",
                    "config",
                ]
            ):
                if log_output:
                    log_output.append("[RULE ENGINE] Pattern matched: QUESTION (platform-related)")
                return await self._handle_help_request(message, context)

        # Error/problem detection
        if any(
            word in message
            for word in ["error", "problem", "broken", "not working", "issue", "bug"]
        ):
            if log_output:
                log_output.append("[RULE ENGINE] Pattern matched: ERROR_REPORT")
            return await self._handle_error_report(message, context)

        # Default response
        if log_output:
            log_output.append("[RULE ENGINE] No pattern matched - using DEFAULT_RESPONSE")
        
        gator_start = random.choice(self.gator_phrases)
        confidence_quote = random.choice(self.gator_confidence)
        return f"{gator_start}, I'm not sure what you're asking about. Be more specific - what do you need help with? Personas? Content? DNS? System status? {confidence_quote} - now give me something to work with here."
    
    async def _generate_response(
        self, message: str, context: Optional[Dict] = None
    ) -> str:
        """Generate Gator's response based on message analysis."""
        
        # Check for action commands first
        action = self._detect_action(message)
        if action:
            # For non-verbose mode, return a simplified action response
            return await self._execute_action_simple(action, message)
        
        # Detect if this is an ACD/system understanding query that needs reasoning
        acd_keywords = [
            "system", "status", "health", "acd", "context", "understanding",
            "error", "fail", "issue", "problem", "what's happening", "what is happening",
            "explain", "analyze", "diagnose", "performance", "activity",
            "why", "how", "what", "tell me about"
        ]
        
        needs_acd_reasoning = any(keyword in message for keyword in acd_keywords)
        
        # Use AI models manager which handles local models
        if self.models_available and self.ai_models:
            try:
                # Check if any text models are actually loaded
                text_models = self.ai_models.available_models.get("text", [])
                loaded_models = [m for m in text_models if m.get("loaded")]
                
                if loaded_models:
                    # Fetch persona information for context
                    personas_info = await self._get_personas_info()
                    
                    # For ACD/system queries, include comprehensive context for reasoning
                    if needs_acd_reasoning:
                        acd_context = await self._build_acd_context_for_reasoning(hours=24)
                        system_prompt = self._build_system_prompt_with_acd(
                            personas_info=personas_info,
                            acd_context=acd_context,
                            include_context=True
                        )
                        max_tokens = 400  # More tokens for detailed reasoning
                    else:
                        system_prompt = self._build_system_prompt(personas_info, include_context=True)
                        max_tokens = 200
                    
                    full_prompt = f"{system_prompt}\n\nUser: {message}\nGator:"
                    
                    llm_response = await self.ai_models.generate_text(
                        full_prompt, 
                        max_tokens=max_tokens, 
                        temperature=0.7 if needs_acd_reasoning else 0.8
                    )
                    
                    if llm_response:
                        return llm_response
                else:
                    logger.info("No text models loaded - using rule-based fallback")
                    
            except Exception as e:
                logger.warning(f"AI model generation failed, falling back to rules: {e}")
        
        # Fallback to rule-based response when no models available
        return await self._generate_rule_based_response(message, context)
    
    async def _execute_action_simple(self, action: Dict, original_message: str) -> str:
        """Execute action and return a simple response (non-verbose mode)."""
        action_type = action["type"]
        query = action.get("query", "")
        
        gator_start = random.choice(self.gator_phrases)
        
        if action_type == "generate_image":
            try:
                if self.ai_models:
                    # Extract prompt
                    clean_prompt = original_message
                    for prefix in self.IMAGE_PROMPT_PREFIXES:
                        if clean_prompt.lower().startswith(prefix):
                            clean_prompt = clean_prompt[len(prefix):].strip()
                            break
                    
                    if not clean_prompt:
                        clean_prompt = self.DEFAULT_IMAGE_PROMPT
                    
                    result = await self.ai_models.generate_image(prompt=clean_prompt)
                    if result and result.get("image_path"):
                        return f"{gator_start}. ðŸŽ¨ Image generated! Saved to: {result['image_path']}"
                    else:
                        return f"{gator_start}. Image generation failed. Check if you have image models loaded."
                else:
                    return f"{gator_start}. No AI models available. Install image models first."
            except Exception as e:
                return f"{gator_start}. Image generation failed: {str(e)}"
        
        elif action_type == "search_models":
            return f"{gator_start}. To search for models, use 'search civitai <query>' or 'search huggingface <query>'. Enable CLI mode for detailed results."
        
        elif action_type == "search_civitai":
            return f"{gator_start}. Enable CLI mode (checkbox) to see CivitAI search results. I'll show you the top models for '{query}'."
        
        elif action_type == "search_huggingface":
            return f"{gator_start}. Enable CLI mode (checkbox) to see HuggingFace search results. I'll find models matching '{query}'."
        
        elif action_type == "install_model":
            return f"{gator_start}. Enable CLI mode (checkbox) to install models. Provide a model ID like 'install model 12345'."
        
        # ACD Understanding actions - use LLM reasoning with system context
        elif action_type in ["system_understanding", "acd_explain", "acd_search", "acd_recall", "system_errors", "domain_info"]:
            # For ACD queries in non-verbose mode, use LLM reasoning with system context
            try:
                if self.models_available and self.ai_models:
                    text_models = self.ai_models.available_models.get("text", [])
                    loaded_models = [m for m in text_models if m.get("loaded")]
                    
                    if loaded_models:
                        personas_info = await self._get_personas_info()
                        acd_context = await self._build_acd_context_for_reasoning(hours=24)
                        system_prompt = self._build_system_prompt_with_acd(
                            personas_info=personas_info,
                            acd_context=acd_context,
                            include_context=True
                        )
                        
                        # Build a specific prompt based on action type
                        action_prompts = {
                            "system_understanding": "Analyze the current system state and give me a concise summary of what's happening, including health status, any issues, and key recommendations.",
                            "acd_explain": f"Explain the ACD system and how it works. What is it tracking right now? {query}",
                            "acd_search": f"Tell me about the recent ACD contexts in the system. What operations have been tracked? {query}",
                            "acd_recall": f"Analyze the ACD activity and tell me what operations the system has been doing. {query}",
                            "system_errors": "Analyze any recent errors in the system. What went wrong and what should be done about it?",
                            "domain_info": f"Tell me about the {query if query else 'system'} domain activity and what's happening in that area.",
                        }
                        
                        user_prompt = action_prompts.get(action_type, original_message)
                        full_prompt = f"{system_prompt}\n\nUser: {user_prompt}\nGator:"
                        
                        llm_response = await self.ai_models.generate_text(
                            full_prompt, 
                            max_tokens=400, 
                            temperature=0.7
                        )
                        
                        if llm_response:
                            return llm_response
                
                # Fallback if LLM not available
                return f"{gator_start}. I need AI models loaded to give you intelligent analysis. Enable CLI mode for detailed raw data, or ask me about specific topics."
                
            except Exception as e:
                logger.warning(f"ACD reasoning failed: {e}")
                return f"{gator_start}. Had trouble analyzing the system. Enable CLI mode for detailed diagnostics, or try again."
        
        return f"{gator_start}. I detected an action but couldn't process it. Try enabling CLI mode for more details."

    async def _handle_help_request(
        self, message: str, context: Optional[Dict] = None
    ) -> str:
        """Handle help requests with specific guidance."""

        # Persona-related help
        if any(
            word in message for word in ["persona", "character", "influencer", "create"]
        ):
            topic = "personas"
            if "create" in message:
                help_text = self.knowledge_base[topic]["create"]
            elif "manage" in message:
                help_text = self.knowledge_base[topic]["manage"]
            else:
                help_text = self.knowledge_base[topic]["content"]

        # NSFW-related help - handle FIRST to prioritize these requests
        elif any(
            word in message for word in ["nsfw", "explicit", "adult", "nude", "naked", "sexy", "erotic"]
        ):
            topic = "nsfw"
            if any(word in message for word in ["prompt", "write", "create", "help"]):
                help_text = self.knowledge_base[topic]["prompts"]
            elif any(word in message for word in ["model", "checkpoint", "lora"]):
                help_text = self.knowledge_base[topic]["models"]
            else:
                help_text = self.knowledge_base[topic]["settings"]

        # Content-related help
        elif any(
            word in message for word in ["content", "generate", "image", "text", "post"]
        ):
            topic = "content"
            if "generate" in message:
                help_text = self.knowledge_base[topic]["generate"]
            elif "manage" in message:
                help_text = self.knowledge_base[topic]["manage"]
            elif "nsfw" in message:
                help_text = self.knowledge_base[topic]["nsfw"]
            else:
                help_text = self.knowledge_base[topic]["quality"]

        # DNS-related help
        elif any(
            word in message for word in ["dns", "domain", "godaddy", "setup", "records"]
        ):
            topic = "dns"
            if "setup" in message:
                help_text = self.knowledge_base[topic]["setup"]
            elif "record" in message:
                help_text = self.knowledge_base[topic]["records"]
            else:
                help_text = self.knowledge_base[topic]["godaddy"]

        # System status help
        elif any(
            word in message for word in ["status", "dashboard", "system", "check"]
        ):
            help_text = "Check the system dashboard for real-time status. Look for green indicators - that means everything's running smooth. Red means problems. You can also check /health endpoint for detailed system status."

        # General troubleshooting
        elif any(
            word in message for word in ["slow", "error", "connection", "not working"]
        ):
            topic = "troubleshooting"
            if "slow" in message:
                help_text = self.knowledge_base[topic]["slow"]
            elif "connection" in message:
                help_text = self.knowledge_base[topic]["connection"]
            else:
                help_text = self.knowledge_base[topic]["errors"]

        else:
            # General help
            help_text = random.choice(self.gator_responses["help_general"])

        gator_start = random.choice(self.gator_phrases)
        return f"{gator_start}. {help_text}"

    async def _handle_error_report(
        self, message: str, context: Optional[Dict] = None
    ) -> str:
        """Handle error reports and provide troubleshooting guidance."""

        gator_start = random.choice(self.gator_responses["error"])

        # Provide specific troubleshooting based on error type
        if "dns" in message:
            advice = "Check your DNS settings and GoDaddy API credentials. Make sure your domain is properly configured."
        elif "persona" in message:
            advice = "Persona problems usually mean bad input data. Check your persona settings and make sure all required fields are filled out."
        elif "content" in message:
            advice = "Content generation issues? Check your AI model settings and make sure your personas are properly configured."
        elif "slow" in message or "loading" in message:
            advice = "Performance issues? Check your system resources - CPU, RAM, and disk space. This platform needs proper hardware."
        else:
            advice = "First, check the system status in the dashboard. Most problems show up there. If it's green, the problem might be on your end."

        confidence_quote = random.choice(self.gator_confidence)
        return f"{gator_start} {advice} {confidence_quote} - and next time, give me more details about what exactly went wrong."

    async def _generate_local_llm_response(
        self, message: str, context: Optional[Dict] = None
    ) -> Optional[str]:
        """Generate response using local LLM models."""
        try:
            if not self.ai_models:
                return None
            
            # Fetch persona information for context
            personas_info = await self._get_personas_info()
            
            # Build system prompt with Gator's persona, NSFW permission, and persona knowledge
            system_prompt = self._build_system_prompt(personas_info, include_context=True)
            
            # Add context if provided
            if context:
                system_prompt += f"\n\nCurrent context: {context}"
            
            # Use local text generation model
            full_prompt = f"{system_prompt}\n\nUser: {message}\nGator:"
            
            # Check if we have any text models available
            if not self.ai_models.available_models.get("text"):
                logger.debug("No local text models available")
                return None
            
            # For now, return None to use cloud/rule-based until we fully initialize models
            # In production, this would call the actual local model
            logger.debug("Local model integration requires full model initialization")
            return None
            
        except Exception as e:
            logger.error(f"Local LLM response generation failed: {e}")
            return None

    async def _generate_cloud_llm_response(
        self, message: str, context: Optional[Dict] = None
    ) -> Optional[str]:
        """Generate response using cloud LLM APIs (OpenAI or Anthropic)."""
        try:
            import httpx
            
            # Fetch persona information for context
            personas_info = await self._get_personas_info()
            
            # Build system prompt with Gator's persona, NSFW permission, and persona knowledge
            system_prompt = self._build_system_prompt(personas_info, include_context=True)
            
            # Add context if provided
            if context:
                system_prompt += f"\n\nCurrent context: {context}"
            
            # Try OpenAI first
            if self.openai_api_key:
                async with httpx.AsyncClient(timeout=30.0) as client:
                    response = await client.post(
                        "https://api.openai.com/v1/chat/completions",
                        headers={
                            "Authorization": f"Bearer {self.openai_api_key}",
                            "Content-Type": "application/json",
                        },
                        json={
                            "model": "gpt-3.5-turbo",
                            "messages": [
                                {"role": "system", "content": system_prompt},
                                {"role": "user", "content": message}
                            ],
                            "temperature": 0.8,
                            "max_tokens": 200,
                        }
                    )
                    
                    if response.status_code == 200:
                        data = response.json()
                        return data["choices"][0]["message"]["content"]
            
            # Try Anthropic if OpenAI failed or not available
            if self.anthropic_api_key:
                async with httpx.AsyncClient(timeout=30.0) as client:
                    response = await client.post(
                        "https://api.anthropic.com/v1/messages",
                        headers={
                            "x-api-key": self.anthropic_api_key,
                            "anthropic-version": "2023-06-01",
                            "Content-Type": "application/json",
                        },
                        json={
                            "model": "claude-3-haiku-20240307",
                            "max_tokens": 200,
                            "system": system_prompt,
                            "messages": [
                                {"role": "user", "content": message}
                            ],
                        }
                    )
                    
                    if response.status_code == 200:
                        data = response.json()
                        return data["content"][0]["text"]
            
            return None
            
        except Exception as e:
            logger.error(f"LLM response generation failed: {e}")
            return None

    def get_conversation_history(self) -> List[Dict[str, str]]:
        """Get the conversation history."""
        return self.conversation_history.copy()

    def clear_conversation_history(self) -> None:
        """Clear the conversation history."""
        self.conversation_history.clear()

    def get_quick_help_topics(self) -> List[Dict[str, str]]:
        """Get a list of quick help topics for the UI."""
        return [
            {"topic": "ðŸ§  System Understanding", "message": "system understanding"},
            {"topic": "ðŸ“Š Check Errors", "message": "show recent errors"},
            {"topic": "ðŸ”„ ACD Contexts", "message": "search contexts"},
            {"topic": "ðŸ’¡ Explain ACD", "message": "explain acd"},
            {"topic": "ðŸŽ¨ Generate Image", "message": "generate an image of a sunset over mountains"},
            {"topic": "ðŸ” Search CivitAI", "message": "search civitai stable diffusion xl"},
            {"topic": "ðŸ¤— Search HuggingFace", "message": "search huggingface diffusion models"},
            {"topic": "ðŸ“¦ Install Model", "message": "How do I install a model?"},
            {"topic": "ðŸŽ­ Creating Personas", "message": "How do I create a new persona?"},
            {"topic": "ðŸ“ Generate Content", "message": "How do I generate content?"},
            {"topic": "ðŸ”§ System Status", "message": "How do I check system status?"},
            {"topic": "â“ Troubleshooting", "message": "Something's not working right"},
        ]


# Global instance for the service
gator_agent = GatorAgentService()
