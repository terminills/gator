"""
Persona Management API Routes

Handles AI persona creation, management, and configuration.
"""

from typing import List, Optional, Dict, Any
from uuid import UUID
from datetime import datetime
import os
import base64
import asyncio
import json

import httpx
from fastapi import APIRouter, Depends, HTTPException, status, Query, UploadFile, File
from pydantic import BaseModel, Field
from sqlalchemy.ext.asyncio import AsyncSession

from backend.database.connection import get_db_session
from backend.models.persona import (
    PersonaCreate,
    PersonaResponse,
    PersonaUpdate,
    BaseImageStatus,
)
from backend.services.persona_service import PersonaService
from backend.services.ai_models import AIModelManager
from backend.config.logging import get_logger

logger = get_logger(__name__)

# Ollama configuration for chat image generation
# These can be overridden via environment variables
OLLAMA_BASE_URL = os.environ.get("OLLAMA_BASE_URL", "http://localhost:11434")
OLLAMA_CONNECT_TIMEOUT = float(os.environ.get("OLLAMA_CONNECT_TIMEOUT", "5.0"))
OLLAMA_GENERATE_TIMEOUT = float(os.environ.get("OLLAMA_GENERATE_TIMEOUT", "60.0"))

# Maximum image upload size (10MB)
MAX_IMAGE_SIZE_BYTES = 10 * 1024 * 1024

router = APIRouter(
    prefix="/api/v1/personas",
    tags=["personas"],
    responses={404: {"description": "Persona not found"}},
)


def get_persona_service(db: AsyncSession = Depends(get_db_session)) -> PersonaService:
    """Dependency injection for PersonaService."""
    return PersonaService(db)


@router.post("/", response_model=PersonaResponse, status_code=status.HTTP_201_CREATED)
async def create_persona(
    persona_data: PersonaCreate,
    persona_service: PersonaService = Depends(get_persona_service),
):
    """
    Create a new AI persona.

    Creates a new persona configuration that will be used for consistent
    character generation across all content. The persona includes appearance,
    personality traits, content themes, and style preferences.

    Args:
        persona_data: Persona configuration data
        persona_service: Injected persona service

    Returns:
        PersonaResponse: Created persona with metadata

    Raises:
        400: Validation error or content moderation failure
        500: Internal server error
    """
    try:
        persona = await persona_service.create_persona(persona_data)
        logger.info(f"Persona created: {persona.id}")
        return persona
    except ValueError as e:
        logger.warning(f"Persona validation failed: {str(e)}")
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=str(e))
    except Exception as e:
        logger.error(f"Failed to create persona: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Internal server error",
        )


@router.get("/", response_model=List[PersonaResponse])
async def list_personas(
    skip: int = Query(0, ge=0, description="Number of personas to skip"),
    limit: int = Query(10, ge=1, le=100, description="Number of personas to return"),
    active_only: bool = Query(True, description="Return only active personas"),
    persona_service: PersonaService = Depends(get_persona_service),
):
    """
    List personas with pagination.

    Retrieves a paginated list of personas with optional filtering
    by active status.

    Args:
        skip: Number of records to skip for pagination
        limit: Maximum number of records to return
        active_only: Whether to return only active personas
        persona_service: Injected persona service

    Returns:
        List[PersonaResponse]: List of personas
    """
    try:
        personas = await persona_service.list_personas(
            skip=skip, limit=limit, active_only=active_only
        )
        return personas
    except Exception as e:
        logger.error(f"Failed to list personas: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Internal server error",
        )


@router.get("/{persona_id}", response_model=PersonaResponse)
async def get_persona(
    persona_id: str,
    persona_service: PersonaService = Depends(get_persona_service),
):
    """
    Get a specific persona by ID.

    Args:
        persona_id: The persona identifier
        persona_service: Injected persona service

    Returns:
        PersonaResponse: The requested persona

    Raises:
        404: Persona not found
        500: Internal server error
    """
    try:
        persona = await persona_service.get_persona(persona_id)
        if not persona:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"Persona {persona_id} not found",
            )
        return persona
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to get persona {persona_id}: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Internal server error",
        )


@router.put("/{persona_id}", response_model=PersonaResponse)
async def update_persona(
    persona_id: str,
    updates: PersonaUpdate,
    persona_service: PersonaService = Depends(get_persona_service),
):
    """
    Update an existing persona.

    Updates the specified persona with new data. All updates go through
    validation and content moderation.

    Args:
        persona_id: The persona to update
        updates: Fields to update
        persona_service: Injected persona service

    Returns:
        PersonaResponse: Updated persona

    Raises:
        404: Persona not found
        400: Validation error
        500: Internal server error
    """
    try:
        persona = await persona_service.update_persona(persona_id, updates)
        if not persona:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"Persona {persona_id} not found",
            )
        logger.info(f"Persona updated {persona_id}")
        return persona
    except HTTPException:
        raise
    except ValueError as e:
        logger.warning(f"Persona update validation failed: {str(e)}")
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=str(e))
    except Exception as e:
        logger.error(f"Failed to update persona {persona_id}: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Internal server error",
        )


@router.delete("/{persona_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_persona(
    persona_id: str,
    persona_service: PersonaService = Depends(get_persona_service),
):
    """
    Delete a persona (soft delete).

    Marks the persona as inactive rather than permanently deleting it.
    This preserves referential integrity and audit trails.

    Args:
        persona_id: The persona to delete
        persona_service: Injected persona service

    Raises:
        404: Persona not found
        500: Internal server error
    """
    try:
        success = await persona_service.delete_persona(persona_id)
        if not success:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"Persona {persona_id} not found",
            )
        logger.info(f"Persona deleted {persona_id}")
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to delete persona {persona_id}: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Internal server error",
        )


@router.post("/{persona_id}/seed-image/upload", response_model=PersonaResponse)
async def upload_seed_image(
    persona_id: str,
    file: UploadFile = File(...),
    persona_service: PersonaService = Depends(get_persona_service),
):
    """
    Upload a seed image for a persona (Method 1: User Upload).

    Accepts binary file data and saves it as the persona's base image.
    Sets base_image_status to DRAFT for review.

    Args:
        persona_id: The persona to upload the image for
        file: The image file (PNG, JPG, WEBP)
        persona_service: Injected persona service

    Returns:
        PersonaResponse: Updated persona with base image

    Raises:
        404: Persona not found
        400: Invalid file format or upload error
        500: Internal server error
    """
    try:
        # Get the persona
        persona = await persona_service.get_persona(persona_id)
        if not persona:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"Persona {persona_id} not found",
            )

        # Validate file type
        allowed_types = ["image/png", "image/jpeg", "image/jpg", "image/webp"]
        if file.content_type not in allowed_types:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"Invalid file type. Allowed: {', '.join(allowed_types)}",
            )

        # Read file data
        image_data = await file.read()
        
        # Log the size immediately after reading
        logger.info(f"Read {len(image_data)} bytes from uploaded file '{file.filename}'")

        # Validate file size (max 10MB)
        max_size = MAX_IMAGE_SIZE_BYTES
        if len(image_data) > max_size:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="File too large. Maximum size is 10MB",
            )

        # Save image to disk
        file_extension = file.filename.split(".")[-1] if "." in file.filename else "png"
        custom_filename = f"persona_{persona_id}_uploaded.{file_extension}"
        
        logger.info(f"Saving image to disk: {len(image_data)} bytes as {custom_filename}")
        image_path = await persona_service._save_image_to_disk(
            persona_id=persona_id, image_data=image_data, filename=custom_filename
        )
        logger.info(f"Image saved successfully to: {image_path}")

        # Update persona with image path and status
        from backend.models.persona import PersonaUpdate

        updates = PersonaUpdate(
            base_image_path=image_path, base_image_status=BaseImageStatus.DRAFT
        )
        updated_persona = await persona_service.update_persona(persona_id, updates)

        logger.info(f"Uploaded seed image for persona {persona_id}")
        return updated_persona

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to upload seed image for persona {persona_id}: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to upload image: {str(e)}",
        )


@router.post("/{persona_id}/seed-image/generate-cloud", response_model=PersonaResponse)
async def generate_seed_image_cloud(
    persona_id: str,
    persona_service: PersonaService = Depends(get_persona_service),
):
    """
    Generate a seed image using DALL-E 3 (Method 2: Cloud Generation).

    Triggers asynchronous generation using the persona's appearance and
    personality via OpenAI DALL-E 3 API. Sets base_image_status to DRAFT.

    Args:
        persona_id: The persona to generate the image for
        persona_service: Injected persona service

    Returns:
        PersonaResponse: Updated persona with generated base image

    Raises:
        404: Persona not found
        400: Missing appearance description or API key
        500: Generation error
    """
    try:
        # Get the persona
        persona = await persona_service.get_persona(persona_id)
        if not persona:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"Persona {persona_id} not found",
            )

        # Check if persona has appearance description
        appearance_prompt = persona.base_appearance_description or persona.appearance
        if not appearance_prompt:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Persona must have an appearance description to generate image",
            )

        # Initialize AI model manager
        ai_manager = AIModelManager()
        await ai_manager.initialize_models()

        # Check if DALL-E is available
        dalle_available = any(
            m.get("name") == "dall-e-3" and m.get("provider") == "openai"
            for m in ai_manager.available_models.get("image", [])
        )

        if not dalle_available:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="DALL-E 3 is not available. Check OPENAI_API_KEY configuration",
            )

        # Generate reference image
        logger.info(f"Generating cloud reference image for persona {persona_id}")
        result = await ai_manager._generate_reference_image_openai(
            appearance_prompt=appearance_prompt,
            personality_context=(
                persona.personality[:200] if persona.personality else None
            ),
            quality="hd",
            size="1024x1024",
        )

        # Save image to disk
        image_path = await persona_service._save_image_to_disk(
            persona_id=persona_id,
            image_data=result["image_data"],
            filename=f"persona_{persona_id}_dalle3.png",
        )

        # Update persona with image path and status
        from backend.models.persona import PersonaUpdate

        updates = PersonaUpdate(
            base_image_path=image_path, base_image_status=BaseImageStatus.DRAFT
        )
        updated_persona = await persona_service.update_persona(persona_id, updates)

        logger.info(f"Generated cloud seed image for persona {persona_id}")

        # Clean up
        await ai_manager.close()

        return updated_persona

    except HTTPException:
        raise
    except Exception as e:
        logger.error(
            f"Failed to generate cloud seed image for persona {persona_id}: {str(e)}"
        )
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to generate image: {str(e)}",
        )


@router.post("/{persona_id}/seed-image/generate-local", response_model=PersonaResponse)
async def generate_seed_image_local(
    persona_id: str,
    persona_service: PersonaService = Depends(get_persona_service),
):
    """
    Generate a seed image using local Stable Diffusion (Method 3: Local Generation).

    Triggers generation using local ROCm/MI25 hardware with Stable Diffusion.
    Can use existing draft image with ControlNet for refinement.
    Sets base_image_status to DRAFT.

    Args:
        persona_id: The persona to generate the image for
        persona_service: Injected persona service

    Returns:
        PersonaResponse: Updated persona with generated base image

    Raises:
        404: Persona not found
        400: Missing appearance description or no local models
        500: Generation error
    """
    try:
        # Get the persona
        persona = await persona_service.get_persona(persona_id)
        if not persona:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"Persona {persona_id} not found",
            )

        # Check if persona has appearance description
        appearance_prompt = persona.base_appearance_description or persona.appearance
        if not appearance_prompt:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Persona must have an appearance description to generate image",
            )

        # Initialize AI model manager
        ai_manager = AIModelManager()
        await ai_manager.initialize_models()

        # Check if local models are available
        local_models = [
            m
            for m in ai_manager.available_models.get("image", [])
            if m.get("provider") == "local" and m.get("loaded")
        ]

        if not local_models:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="No local image generation models available",
            )

        # Generate reference image (use existing base_image_path for ControlNet if available)
        logger.info(f"Generating local reference image for persona {persona_id}")
        result = await ai_manager._generate_reference_image_local(
            appearance_prompt=appearance_prompt,
            personality_context=(
                persona.personality[:200] if persona.personality else None
            ),
            reference_image_path=(
                persona.base_image_path if persona.base_image_path else None
            ),
            width=1024,
            height=1024,
            num_inference_steps=50,
        )

        # Save image to disk
        image_path = await persona_service._save_image_to_disk(
            persona_id=persona_id,
            image_data=result["image_data"],
            filename=f"persona_{persona_id}_local.png",
        )

        # Update persona with image path and status
        from backend.models.persona import PersonaUpdate

        updates = PersonaUpdate(
            base_image_path=image_path, base_image_status=BaseImageStatus.DRAFT
        )
        updated_persona = await persona_service.update_persona(persona_id, updates)

        logger.info(f"Generated local seed image for persona {persona_id}")

        # Clean up
        await ai_manager.close()

        return updated_persona

    except HTTPException:
        raise
    except Exception as e:
        logger.error(
            f"Failed to generate local seed image for persona {persona_id}: {str(e)}"
        )
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to generate image: {str(e)}",
        )


@router.post("/{persona_id}/seed-image/approve", response_model=PersonaResponse)
async def approve_seed_image(
    persona_id: str,
    persona_service: PersonaService = Depends(get_persona_service),
):
    """
    Approve the baseline seed image for a persona.

    Sets base_image_status to APPROVED and appearance_locked to True,
    enabling visual consistency features for all future content generation.

    Args:
        persona_id: The persona to approve the image for
        persona_service: Injected persona service

    Returns:
        PersonaResponse: Updated persona with approved base image

    Raises:
        404: Persona not found
        400: No base image to approve
        500: Internal server error
    """
    try:
        # Approve the baseline image
        persona = await persona_service.approve_baseline(persona_id)

        if not persona:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"Persona {persona_id} not found",
            )

        logger.info(f"Approved seed image for persona {persona_id}")
        return persona

    except ValueError as e:
        logger.warning(f"Cannot approve baseline for persona {persona_id}: {str(e)}")
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=str(e))
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to approve seed image for persona {persona_id}: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Internal server error",
        )


# Expanded base image types for complete physical appearance locking and LoRA training
# Organized in 3 phases as recommended for optimal ControlNet progression
#
# Phase 1 - STRUCTURAL SET: Establishes physical identity and body structure
# Phase 2 - ACTION SET: Dynamic poses for LoRA training (teaches clothing/muscle movement)
# Phase 3 - BACKGROUND VARIATION: Character in varied environments (regularization)

# Phase 1: Structural Set - Identity and body structure
STRUCTURAL_IMAGE_TYPES = [
    {
        "id": "front_headshot",
        "label": "Front Head Shot",
        "phase": "structural",
        "order": 1,
        "prompt_addition": "close-up portrait, front facing headshot, face centered, shoulders visible, looking directly at camera, detailed facial features, neutral expression, studio lighting, white background",
        "controlnet_type": "none",  # Starting point - no reference needed
        "caption_tags": "from front, portrait, simple background, white background",
    },
    {
        "id": "side_headshot",
        "label": "Side Head Shot",
        "phase": "structural",
        "order": 2,
        "prompt_addition": "close-up portrait, side profile headshot, face in profile, ear visible, detailed facial features, studio lighting, white background",
        "controlnet_type": "canny",  # Use canny from front headshot
        "caption_tags": "from side, profile, portrait, simple background, white background",
    },
    {
        "id": "right_hand",
        "label": "Right Hand Detail",
        "phase": "structural",
        "order": 3,
        "prompt_addition": "detailed close-up of right hand, palm facing camera, fingers spread, detailed hand anatomy, clean nails, studio lighting, white background",
        "controlnet_type": "none",
        "caption_tags": "hand focus, right hand, fingers, simple background",
    },
    {
        "id": "left_hand",
        "label": "Left Hand Detail",
        "phase": "structural",
        "order": 4,
        "prompt_addition": "detailed close-up of left hand, palm facing camera, fingers spread, detailed hand anatomy, clean nails, studio lighting, white background",
        "controlnet_type": "canny",  # Use right hand as reference
        "caption_tags": "hand focus, left hand, fingers, simple background",
    },
    {
        "id": "bust",
        "label": "Bust View",
        "phase": "structural",
        "order": 5,
        "prompt_addition": "bust shot, upper body, chest and shoulders, facing camera, detailed torso anatomy, studio lighting, white background",
        "controlnet_type": "canny",  # Use headshot as reference
        "caption_tags": "upper body, bust, from front, simple background",
    },
    {
        "id": "full_frontal",
        "label": "Full Frontal View",
        "phase": "structural",
        "order": 6,
        "prompt_addition": "full body shot, front view, A-pose or T-pose, standing straight, arms slightly away from body, feet shoulder width apart, anatomically correct proportions, studio lighting, white background",
        "controlnet_type": "openpose",  # Use OpenPose for body structure
        "caption_tags": "full body, standing, from front, a-pose, simple background",
    },
    {
        "id": "side_profile",
        "label": "Side Profile View",
        "phase": "structural",
        "order": 7,
        "prompt_addition": "full body shot, side profile view, standing straight, arms at sides, anatomically correct proportions, studio lighting, white background",
        "controlnet_type": "canny",  # Use full frontal as reference
        "caption_tags": "full body, standing, from side, profile, simple background",
    },
    {
        "id": "rear_view",
        "label": "Rear View",
        "phase": "structural",
        "order": 8,
        "prompt_addition": "full body shot, rear view from behind, standing straight, anatomically correct proportions, studio lighting, white background",
        "controlnet_type": "canny",  # Use side profile as reference
        "caption_tags": "full body, standing, from behind, back view, simple background",
    },
]

# Phase 2: Action Set - Dynamic poses for LoRA training
ACTION_IMAGE_TYPES = [
    {
        "id": "compression_pose",
        "label": "Compression Pose (Sitting/Crouching)",
        "phase": "action",
        "order": 9,
        "prompt_addition": "full body, sitting or crouching pose, knees bent, natural fabric folds and creases, relaxed posture, studio lighting, white background",
        "controlnet_type": "openpose",
        "caption_tags": "full body, sitting, crouching, bent knees, simple background",
        "training_purpose": "Teaches model how pants/fabric fold when compressed",
    },
    {
        "id": "extension_pose",
        "label": "Extension Pose (Reaching/Running)",
        "phase": "action",
        "order": 10,
        "prompt_addition": "full body, dynamic pose, arms reaching upward or running motion, extended limbs, visible armpit and shoulder connection, studio lighting, white background",
        "controlnet_type": "openpose",
        "caption_tags": "full body, reaching, arms up, dynamic pose, simple background",
        "training_purpose": "Teaches model how armpit/shoulder area connects during extension",
    },
    {
        "id": "twist_pose",
        "label": "Twist Pose (Looking Back)",
        "phase": "action",
        "order": 11,
        "prompt_addition": "full body, twisted pose, looking back over shoulder, torso rotation, neck turned, dynamic angle, studio lighting, white background",
        "controlnet_type": "openpose",
        "caption_tags": "full body, looking back, twisted torso, over shoulder, simple background",
        "training_purpose": "Teaches model neck rotation and torso twist mechanics",
    },
]

# Phase 3: Background Variation - Character in varied environments (regularization)
BACKGROUND_IMAGE_TYPES = [
    {
        "id": "complex_bg_1",
        "label": "Complex Background 1 (Urban)",
        "phase": "background",
        "order": 12,
        "prompt_addition": "full body, standing naturally, urban environment, city street or cafe setting, complex background with depth, natural lighting",
        "controlnet_type": "canny",  # Use full frontal as structure reference
        "caption_tags": "full body, standing, urban background, city, outdoors",
        "training_purpose": "Regularization - teaches character can exist in urban environments",
    },
    {
        "id": "complex_bg_2",
        "label": "Complex Background 2 (Nature)",
        "phase": "background",
        "order": 13,
        "prompt_addition": "full body, standing naturally, nature environment, forest or park setting, complex background with foliage, natural lighting, outdoors",
        "controlnet_type": "canny",
        "caption_tags": "full body, standing, nature background, forest, outdoors",
        "training_purpose": "Regularization - teaches character can exist in natural environments",
    },
    {
        "id": "complex_bg_3",
        "label": "Complex Background 3 (Interior)",
        "phase": "background",
        "order": 14,
        "prompt_addition": "full body, standing or sitting naturally, indoor environment, modern interior or home setting, complex background with furniture, interior lighting",
        "controlnet_type": "canny",
        "caption_tags": "full body, indoors, interior background, room, home",
        "training_purpose": "Regularization - teaches character can exist in interior spaces",
    },
]

# Combined list of all base image types
BASE_IMAGE_TYPES = STRUCTURAL_IMAGE_TYPES + ACTION_IMAGE_TYPES + BACKGROUND_IMAGE_TYPES

# Total count of images in the expanded system (for LoRA training completeness check)
TOTAL_EXPANDED_IMAGE_COUNT = len(BASE_IMAGE_TYPES)  # Currently 14


@router.post("/generate-sample-images")
async def generate_sample_images(
    appearance: str = Query(
        ..., description="Appearance description for image generation"
    ),
    personality: Optional[str] = Query(
        None, description="Personality context for image generation"
    ),
    resolution: Optional[str] = Query(
        "1024x1024",
        description="Image resolution (e.g., '1024x1024', '720x1280', '1920x1080')",
    ),
    quality: Optional[str] = Query(
        "standard", description="Generation quality: draft, standard, high, premium"
    ),
    style: Optional[str] = Query(
        "photorealistic",
        description="Image generation style: photorealistic, anime, cartoon, artistic, 3d_render, fantasy, cinematic",
    ),
    nsfw_model: Optional[str] = Query(
        None,
        description="Optional NSFW model preference for better human body/anatomy details (e.g., 'realistic', 'photon')",
    ),
    phases: Optional[str] = Query(
        "structural",
        description=(
            "Which image phases to generate. Options: "
            "'structural' (8 identity/body images), "
            "'action' (3 dynamic poses for LoRA training), "
            "'background' (3 environment variations for regularization), "
            "'all' (all 14 images)"
        ),
    ),
    persona_service: PersonaService = Depends(get_persona_service),
) -> Dict[str, Any]:
    """
    Generate expanded base images for persona physical appearance locking and LoRA training.

    This endpoint generates images in 3 phases based on the recommended ControlNet workflow:

    **Phase 1 - STRUCTURAL SET (8 images):**
    - front_headshot, side_headshot: Facial identity
    - right_hand, left_hand: Hand structure (often problematic in AI generation)
    - bust: Upper body detail
    - full_frontal (A-pose/T-pose): Full body structure
    - side_profile, rear_view: 360Â° body coverage

    **Phase 2 - ACTION SET (3 images):**
    - compression_pose: Sitting/crouching (teaches fabric folds)
    - extension_pose: Reaching/running (teaches shoulder/armpit connection)
    - twist_pose: Looking back (teaches neck/torso rotation)

    **Phase 3 - BACKGROUND VARIATION (3 images):**
    - complex_bg_1: Urban environment
    - complex_bg_2: Nature environment
    - complex_bg_3: Interior environment
    (Prevents "white room" problem in LoRA training)

    The images are generated in sequence using ControlNet for structural consistency,
    with each image potentially using the previous as a reference for visual coherence.

    **Captioning for LoRA Training:**
    Each image includes `caption_tags` metadata following the rule:
    - PERMANENT character features (face, body) = NOT tagged (inherent to character)
    - TEMPORARY aspects (pose, background) = TAGGED (so LoRA learns to vary them)

    Note: NSFW-capable models are preferred because they are better trained on human
    anatomy and physical details, resulting in more accurate body representations.

    Args:
        appearance: Physical appearance description
        personality: Optional personality traits
        resolution: Image resolution in format "widthxheight"
        quality: Generation quality preset (draft, standard, high, premium)
        style: Image generation style
        nsfw_model: Optional preferred NSFW model name for better anatomy rendering
        phases: Which phases to generate (structural, action, background, all, legacy)
        persona_service: Injected persona service

    Returns:
        Dict with:
        - 'images': Array of generated images with id, label, data_url, caption_tags, etc.
        - 'count': Number of images generated
        - 'phases_generated': List of phases that were generated
        - 'training_ready': Whether the set is complete for LoRA training

    Raises:
        400: Missing appearance or no image generation available
        500: Generation error
    """
    try:
        if not appearance or len(appearance.strip()) < 10:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Appearance description must be at least 10 characters",
            )

        # Determine which image types to generate based on phases parameter
        phases_lower = phases.lower() if phases else "structural"
        image_types_to_generate = []
        phases_generated = []

        if phases_lower == "all":
            image_types_to_generate = BASE_IMAGE_TYPES
            phases_generated = ["structural", "action", "background"]
        else:
            # Parse comma-separated phases
            requested_phases = [p.strip().lower() for p in phases_lower.split(",")]
            for phase in requested_phases:
                if phase == "structural":
                    image_types_to_generate.extend(STRUCTURAL_IMAGE_TYPES)
                    phases_generated.append("structural")
                elif phase == "action":
                    image_types_to_generate.extend(ACTION_IMAGE_TYPES)
                    phases_generated.append("action")
                elif phase == "background":
                    image_types_to_generate.extend(BACKGROUND_IMAGE_TYPES)
                    phases_generated.append("background")
            
            # Default to structural if nothing matched
            if not image_types_to_generate:
                image_types_to_generate = STRUCTURAL_IMAGE_TYPES
                phases_generated = ["structural"]

        # Sort by order to ensure proper ControlNet chaining
        image_types_to_generate = sorted(image_types_to_generate, key=lambda x: x.get("order", 0))

        # Initialize AI model manager
        ai_manager = AIModelManager()
        await ai_manager.initialize_models()

        # Check what's available - prefer local models (free, no API costs)
        local_models = [
            m
            for m in ai_manager.available_models.get("image", [])
            if m.get("provider") == "local" and m.get("loaded")
        ]

        # Check for local models - required for ControlNet chaining
        if not local_models:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Local Stable Diffusion models are required for base image generation with ControlNet chaining. "
                       "Cloud APIs like DALL-E cannot provide the visual consistency needed for body reference images. "
                       "Please install Stable Diffusion XL locally.",
            )

        # Parse resolution
        try:
            width, height = map(int, resolution.split("x"))
            if width < 256 or width > 4096 or height < 256 or height > 4096:
                raise ValueError("Resolution dimensions must be between 256 and 4096")
        except (ValueError, AttributeError):
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Invalid resolution format. Use 'widthxheight' (e.g., '1024x1024', '720x1280', '1920x1080')",
            )

        # Map quality to local generation steps
        quality_steps_map = {"draft": 20, "standard": 30, "high": 50, "premium": 80}
        num_steps = quality_steps_map.get(quality.lower(), 30)

        total_images = len(image_types_to_generate)
        logger.info(f"Generating {total_images} base images with appearance: {appearance[:50]}...")
        logger.info(f"Resolution: {width}x{height}, Quality: {quality}, Style: {style}")
        logger.info(f"Phases: {', '.join(phases_generated)}")
        logger.info("Using sequential ControlNet chain for maximum visual consistency")

        # Generate images using sequential ControlNet chain
        # Each image uses the previous one as reference for progressive consistency
        images = []
        current_reference_path = None  # Chain: each image becomes reference for the next
        temp_files = []  # Track temp files for cleanup

        logger.info("Using local Stable Diffusion models with ControlNet for generation")

        # Get available GPUs
        from backend.services.gpu_monitoring_service import (
            get_gpu_monitoring_service,
        )
        import tempfile
        import os

        gpu_service = get_gpu_monitoring_service()
        available_gpus = await gpu_service.get_available_gpus()

        if available_gpus:
            logger.info(
                f"Distributing image generation across {len(available_gpus)} GPU(s): {available_gpus}"
            )

        try:
            # Sequential chain generation: each image becomes reference for the next
            for i, image_type in enumerate(image_types_to_generate):
                try:
                    specific_prompt = f"{appearance}, {image_type['prompt_addition']}"
                    
                    # Select GPU in round-robin fashion
                    device_id = available_gpus[i % len(available_gpus)] if available_gpus else None
                    
                    # Determine generation mode based on controlnet_type and position in chain
                    # controlnet_type values:
                    #   - "none": Pure text2img, no reference (e.g., front_headshot, right_hand)
                    #   - "canny": Use Canny edge detection from reference image
                    #   - "openpose": Use OpenPose skeleton from reference image
                    # Note: If current_reference_path is None, we fall back to text2img
                    # regardless of controlnet_type to avoid errors
                    controlnet_type = image_type.get("controlnet_type", "none")
                    
                    if controlnet_type == "none" or current_reference_path is None:
                        # Pure text2img - starting point or no ControlNet needed
                        # Images with controlnet_type="none" (e.g., front_headshot, right_hand)
                        # are generated fresh without reference to establish identity
                        logger.info(f"STEP {i+1}/{total_images}: Generating {image_type['label']} (text2img)")
                        result = await ai_manager._generate_reference_image_local(
                            appearance_prompt=specific_prompt,
                            personality_context=personality[:200] if personality else None,
                            reference_image_path=None,
                            width=width,
                            height=height,
                            num_inference_steps=num_steps,
                            device_id=device_id,
                            image_style=style,
                            prefer_anatomy_model=True,
                            nsfw_model_pref=nsfw_model,
                        )
                    else:
                        # Use ControlNet/img2img for structural guidance and appearance consistency
                        # Strength varies based on the type of transformation needed
                        phase = image_type.get("phase", "structural")
                        
                        if phase == "structural":
                            # Structural images need high consistency
                            img2img_strength = 0.50
                        elif phase == "action":
                            # Action poses need more freedom for dynamic movement
                            img2img_strength = 0.60
                        elif phase == "background":
                            # Background variations need most freedom
                            img2img_strength = 0.65
                        else:
                            img2img_strength = 0.55
                        
                        logger.info(
                            f"STEP {i+1}/{total_images}: Generating {image_type['label']} "
                            f"({controlnet_type}, strength={img2img_strength})"
                        )
                        
                        result = await ai_manager._generate_reference_image_local(
                            appearance_prompt=specific_prompt,
                            personality_context=personality[:200] if personality else None,
                            reference_image_path=current_reference_path,
                            width=width,
                            height=height,
                            num_inference_steps=num_steps,
                            device_id=device_id,
                            image_style=style,
                            prefer_anatomy_model=True,
                            nsfw_model_pref=nsfw_model,
                            img2img_strength=img2img_strength,
                            use_controlnet=(controlnet_type in ["canny", "openpose"]),
                        )

                    # Save this image as reference for the next in the chain
                    with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as tmp_file:
                        tmp_file.write(result["image_data"])
                        current_reference_path = tmp_file.name
                        temp_files.append(tmp_file.name)
                        logger.info(f"Saved {image_type['label']} as chain reference: {current_reference_path}")

                    # Convert to base64 data URL
                    base64_image = base64.b64encode(result["image_data"]).decode("utf-8")
                    data_url = f"data:image/png;base64,{base64_image}"

                    # Build image response with training metadata
                    image_response = {
                        "id": image_type["id"],
                        "type": image_type["id"],
                        "label": image_type["label"],
                        "phase": image_type.get("phase", "unknown"),
                        "order": image_type.get("order", i + 1),
                        "data_url": data_url,
                        "size": len(result["image_data"]),
                        "gpu_id": device_id if device_id is not None else "auto",
                        "chain_position": i + 1,
                        # LoRA training metadata
                        "caption_tags": image_type.get("caption_tags", ""),
                        "controlnet_type": image_type.get("controlnet_type", "none"),
                    }
                    
                    # Add training purpose if available (for action/background phases)
                    if "training_purpose" in image_type:
                        image_response["training_purpose"] = image_type["training_purpose"]

                    images.append(image_response)

                    logger.info(f"Generated {image_type['label']} ({i+1}/{total_images}) - chain reference updated")

                except Exception as e:
                    error_str = str(e)
                    logger.warning(f"Failed to generate {image_type['label']}: {error_str}")
                    
                    # Check for CUDA/ROCm compatibility issues and provide actionable guidance
                    if "libcudart" in error_str or "xformers" in error_str.lower():
                        logger.warning(
                            "This error is likely caused by xFormers CUDA incompatibility on a ROCm system. "
                            "To fix: Call POST /setup/xformers/uninstall or run 'pip uninstall xformers'"
                        )
                    
                    # If an image in the chain fails, try to continue with text2img for remaining
                    current_reference_path = None

        finally:
            # Always clean up temporary files, even on exception
            for temp_file in temp_files:
                try:
                    os.unlink(temp_file)
                except Exception as e:
                    logger.warning(f"Failed to clean up temp file {temp_file}: {e}")
            if temp_files:
                logger.info(f"Cleaned up {len(temp_files)} temporary chain reference files")

        # Clean up AI manager
        await ai_manager.close()

        if len(images) == 0:
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="Failed to generate any base images. Check that local Stable Diffusion models are properly loaded.",
            )

        # Determine if the generated set is complete for LoRA training
        # Complete set requires: structural + action + background (all images from expanded system)
        training_ready = (
            len(images) >= TOTAL_EXPANDED_IMAGE_COUNT and 
            "structural" in phases_generated and 
            "action" in phases_generated and 
            "background" in phases_generated
        )

        logger.info(f"Successfully generated {len(images)} base images with ControlNet chain")

        return {
            "images": images,
            "count": len(images),
            "phases_generated": phases_generated,
            "training_ready": training_ready,
            "training_guidance": {
                "captioning_rule": "PERMANENT features (face, body) = NOT tagged. TEMPORARY aspects (pose, background) = TAGGED.",
                "complete_for_lora": training_ready,
                "recommended_next": (
                    "Ready for LoRA training!" if training_ready
                    else f"Add phases: {', '.join(set(['structural', 'action', 'background']) - set(phases_generated))}"
                ),
            },
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to generate sample images: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to generate images: {str(e)}",
        )


@router.post(
    "/random", response_model=PersonaResponse, status_code=status.HTTP_201_CREATED
)
async def create_random_persona(
    generate_images: bool = Query(
        False,
        description="If true, generates 4 sample images and auto-selects the first one",
    ),
    resolution: Optional[str] = Query(
        "1024x1024", description="Image resolution if generate_images is true"
    ),
    quality: Optional[str] = Query(
        "standard", description="Generation quality if generate_images is true"
    ),
    persona_service: PersonaService = Depends(get_persona_service),
):
    """
    Create a random persona with randomized appearance, personality, and themes.

    This endpoint generates a complete persona with randomly selected attributes,
    perfect for quick testing, experimentation, or creative inspiration.

    Args:
        generate_images: If true, automatically generates preview images
        resolution: Image resolution for preview generation
        quality: Quality preset for image generation
        persona_service: Injected persona service

    Returns:
        PersonaResponse: The created random persona

    Raises:
        500: Generation error
    """
    try:
        from backend.services.ai_persona_generator import get_ai_persona_generator

        # Generate AI-powered persona configuration using llama.cpp
        # This creates more coherent and realistic personas than random templates
        ai_generator = get_ai_persona_generator()
        random_config = await ai_generator.generate_persona(
            use_ai=True  # Use llama.cpp for AI-powered generation
        )

        logger.info(f"Creating AI-generated persona: {random_config['name']}")

        # Create persona data object with all fields including soul fields
        persona_data = PersonaCreate(
            name=random_config["name"],
            appearance=random_config["appearance"],
            personality=random_config["personality"],
            content_themes=random_config["content_themes"],
            style_preferences=random_config.get("style_preferences", {}),
            default_content_rating=random_config["default_content_rating"],
            allowed_content_ratings=random_config["allowed_content_ratings"],
            platform_restrictions=random_config.get("platform_restrictions", {}),
            is_active=random_config.get("is_active", True),
            # Soul Fields - Origin & Demographics
            hometown=random_config.get("hometown"),
            current_location=random_config.get("current_location"),
            generation_age=random_config.get("generation_age"),
            education_level=random_config.get("education_level"),
            # Soul Fields - Psychological Profile
            mbti_type=random_config.get("mbti_type"),
            enneagram_type=random_config.get("enneagram_type"),
            political_alignment=random_config.get("political_alignment"),
            risk_tolerance=random_config.get("risk_tolerance"),
            optimism_cynicism_scale=random_config.get("optimism_cynicism_scale"),
            # Soul Fields - Voice & Speech Patterns
            linguistic_register=random_config.get("linguistic_register"),
            typing_quirks=random_config.get("typing_quirks", {}),
            signature_phrases=random_config.get("signature_phrases", []),
            trigger_topics=random_config.get("trigger_topics", []),
            # Soul Fields - Backstory & Lore
            day_job=random_config.get("day_job"),
            war_story=random_config.get("war_story"),
            vices_hobbies=random_config.get("vices_hobbies", []),
            # Soul Fields - Anti-Pattern
            forbidden_phrases=random_config.get("forbidden_phrases", []),
            warmth_level=random_config.get("warmth_level"),
            patience_level=random_config.get("patience_level"),
        )

        # Create the persona
        persona = await persona_service.create_persona(persona_data)

        # Optionally generate and set preview images
        if generate_images:
            try:
                logger.info(
                    f"Generating preview images for random persona {persona.id}"
                )

                # Initialize AI model manager
                ai_manager = AIModelManager()
                await ai_manager.initialize_models()

                # Check for local models
                local_models = [
                    m
                    for m in ai_manager.available_models.get("image", [])
                    if m.get("provider") == "local" and m.get("loaded")
                ]

                if local_models:
                    # Parse resolution
                    width, height = map(int, resolution.split("x"))

                    # Map quality to steps
                    quality_steps = {
                        "draft": 20,
                        "standard": 30,
                        "high": 50,
                        "premium": 80,
                    }
                    num_steps = quality_steps.get(quality.lower(), 30)

                    # Generate first image only (faster for random personas)
                    result = await ai_manager._generate_reference_image_local(
                        appearance_prompt=persona.appearance,
                        personality_context=persona.personality[:200],
                        reference_image_path=None,
                        width=width,
                        height=height,
                        num_inference_steps=num_steps,
                    )

                    # Save and set as base image
                    image_path = await persona_service._save_image_to_disk(
                        persona_id=str(persona.id),
                        image_data=result["image_data"],
                        filename=f"persona_{persona.id}_random.png",
                    )

                    # Update persona with image
                    from backend.models.persona import PersonaUpdate

                    updates = PersonaUpdate(
                        base_image_path=image_path,
                        base_image_status=BaseImageStatus.APPROVED,
                        appearance_locked=True,
                    )
                    persona = await persona_service.update_persona(
                        str(persona.id), updates
                    )

                    logger.info(
                        f"Generated and set base image for random persona {persona.id}"
                    )

                    await ai_manager.close()
                else:
                    logger.warning("No local models available for image generation")

            except Exception as e:
                logger.error(f"Failed to generate images for random persona: {str(e)}")
                # Don't fail the whole operation if image generation fails

        logger.info(f"Random persona created: {persona.id} - {persona.name}")
        return persona

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to create random persona: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to create random persona: {str(e)}",
        )


class GenerateSoulFieldsRequest(BaseModel):
    """Request model for generating soul fields."""
    
    name: str = Field(..., description="Persona name for context")
    appearance: str = Field(default="", description="Appearance description for context")
    personality: str = Field(default="", description="Personality description for context")
    content_rating: str = Field(
        default="sfw",
        description="Content rating: sfw, moderate, or nsfw - affects generation style"
    )


class GenerateSoulFieldsResponse(BaseModel):
    """Response model for generated soul fields."""
    
    # Origin & Demographics
    hometown: Optional[str] = None
    current_location: Optional[str] = None
    generation_age: Optional[str] = None
    education_level: Optional[str] = None
    
    # Psychological Profile
    mbti_type: Optional[str] = None
    enneagram_type: Optional[str] = None
    political_alignment: Optional[str] = None
    risk_tolerance: Optional[str] = None
    optimism_cynicism_scale: Optional[int] = None
    
    # Voice & Speech Patterns
    linguistic_register: Optional[str] = None
    typing_quirks: Optional[Dict[str, Any]] = None
    signature_phrases: Optional[List[str]] = None
    trigger_topics: Optional[List[str]] = None
    
    # Backstory & Lore
    day_job: Optional[str] = None
    war_story: Optional[str] = None
    vices_hobbies: Optional[List[str]] = None
    
    # Anti-Pattern
    forbidden_phrases: Optional[List[str]] = None
    warmth_level: Optional[str] = None
    patience_level: Optional[str] = None
    
    # Generation metadata
    generation_method: str = "ollama_ai"
    model_used: Optional[str] = None


@router.post("/generate-soul-fields", response_model=GenerateSoulFieldsResponse)
async def generate_soul_fields(
    request: GenerateSoulFieldsRequest,
):
    """
    Generate soul fields for a persona using AI (Ollama with dolphin-mixtral).
    
    This endpoint generates comprehensive soul fields based on the persona's name,
    appearance, personality, and content rating. The generated fields will match
    the tone appropriate for the content rating:
    
    - **SFW**: Family-friendly, professional persona attributes
    - **Moderate**: More edgy, adult humor, but not explicit
    - **NSFW**: Uninhibited, adult-oriented persona attributes
    
    Args:
        request: Persona context and content rating for generation
        
    Returns:
        GenerateSoulFieldsResponse: Generated soul fields ready to populate form
        
    Raises:
        503: Ollama not available
        500: Generation error
    """
    try:
        # Ollama configuration
        OLLAMA_BASE_URL = "http://localhost:11434"
        
        # Check if Ollama is available
        try:
            async with httpx.AsyncClient(timeout=5.0) as client:
                response = await client.get(f"{OLLAMA_BASE_URL}/api/tags")
                if response.status_code != 200:
                    raise HTTPException(
                        status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                        detail="Ollama is not available. Please ensure Ollama is running with dolphin-mixtral model.",
                    )
                
                data = response.json()
                available_models = [m["name"] for m in data.get("models", [])]
                
                # Find best model
                preferred_models = [
                    "dolphin-mixtral",
                    "dolphin-mixtral:8x7b",
                    "dolphin-mistral",
                    "llama3.1:8b",
                    "llama3:8b",
                    "mistral",
                ]
                
                model_to_use = None
                for preferred in preferred_models:
                    for available in available_models:
                        if preferred in available.lower():
                            model_to_use = available
                            break
                    if model_to_use:
                        break
                
                if not model_to_use and available_models:
                    model_to_use = available_models[0]
                
                if not model_to_use:
                    raise HTTPException(
                        status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                        detail="No suitable models found in Ollama. Please install dolphin-mixtral.",
                    )
                    
        except httpx.RequestError:
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail="Cannot connect to Ollama. Please ensure Ollama is running on localhost:11434",
            )
        
        # Build content-rating-aware prompt
        rating = request.content_rating.lower()
        
        rating_guidance = ""
        if rating == "nsfw":
            rating_guidance = """
This persona is for ADULT/NSFW content. Generate edgy, uninhibited, provocative attributes.
- They can have explicit interests, risquÃ© humor, and adult-oriented backstory
- Signature phrases can include innuendo or adult references
- Hobbies can include adult activities
- They should feel like a confident, uninhibited adult content creator
- NO corporate speak, NO sanitized language - make them feel REAL and SPICY
"""
        elif rating == "moderate":
            rating_guidance = """
This persona is for MODERATE/edgy content. Generate slightly edgy, adult-humor-friendly attributes.
- They can have mild adult humor, occasional crude jokes
- More rebellious or edgy personality traits
- Think "R-rated movie" level - suggestive but not explicit
- They should feel like a fun, slightly rebellious influencer
"""
        else:  # SFW
            rating_guidance = """
This persona is for SFW/family-friendly content. Generate wholesome, professional attributes.
- Keep everything appropriate for general audiences
- Focus on positive, inspiring, or entertaining traits
- No crude humor or adult references
- They should feel like a mainstream, brand-safe influencer
"""

        prompt = f"""You are an expert at creating realistic AI influencer personas that feel like real people.

Generate soul field attributes for this persona:
Name: {request.name}
Appearance: {request.appearance or 'Not specified'}
Personality: {request.personality or 'Not specified'}

{rating_guidance}

# CRITICAL RULES
1. Make them feel HUMAN, not like an AI or customer service bot
2. Give them strong opinions, quirks, and personality
3. Everything should feel internally consistent with the name/appearance/personality
4. The forbidden_phrases should include things this specific persona would NEVER say
5. Signature phrases should feel natural for their background and style

# OUTPUT FORMAT - Respond with ONLY valid JSON (no markdown, no explanation):

{{
  "hometown": "specific city/region they're from with character",
  "current_location": "where they live now and why they moved/stayed",
  "generation_age": "generation with age and context (e.g., 'Gen Z - 24, grew up online')",
  "education_level": "their education with personality (e.g., 'State school dropout, self-taught')",
  
  "mbti_type": "XXXX - The Label (e.g., 'ESTP - The Entrepreneur')",
  "enneagram_type": "Type X - The Label (e.g., 'Type 7 - The Enthusiast')",
  "political_alignment": "their worldview in their own words",
  "risk_tolerance": "their attitude in their own voice",
  "optimism_cynicism_scale": 7,
  
  "linguistic_register": "gen_z|millennial|southern|tech_bro|street|corporate|academic|blue_collar",
  "typing_quirks": {{
    "capitalization": "how they type (all lowercase, normal, RANDOM CAPS)",
    "emoji_usage": "none|minimal|moderate|heavy",
    "punctuation": "their punctuation style"
  }},
  "signature_phrases": ["phrase 1", "phrase 2", "phrase 3", "phrase 4", "phrase 5"],
  "trigger_topics": ["topic that fires them up 1", "topic 2", "topic 3"],
  
  "day_job": "what they do for work/money",
  "war_story": "one defining life moment that shaped who they are",
  "vices_hobbies": ["hobby 1", "hobby 2", "hobby 3", "hobby 4"],
  
  "forbidden_phrases": ["phrase they'd NEVER say 1", "phrase 2", "phrase 3", "phrase 4", "phrase 5"],
  "warmth_level": "cold|neutral|warm|buddy",
  "patience_level": "short_fuse|normal|patient|infinite"
}}

JSON:"""

        # Generate with Ollama (60 second timeout for reasonable user experience)
        logger.info(f"Generating soul fields for '{request.name}' with {model_to_use} (rating: {rating})")
        
        async with httpx.AsyncClient(timeout=60.0) as client:
            response = await client.post(
                f"{OLLAMA_BASE_URL}/api/generate",
                json={
                    "model": model_to_use,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.9,
                        "top_p": 0.95,
                        "num_predict": 2000,
                    }
                }
            )
            
            if response.status_code != 200:
                raise HTTPException(
                    status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                    detail=f"Ollama generation failed with status {response.status_code}",
                )
            
            output = response.json().get("response", "")
        
        # Parse JSON from output - find outermost { } block
        start_idx = output.find('{')
        end_idx = output.rfind('}')
        
        if start_idx == -1 or end_idx == -1:
            logger.error(f"No JSON found in Ollama output: {output[:500]}")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="Failed to parse AI response - no valid JSON found",
            )
        
        json_str = output[start_idx:end_idx + 1]
        
        try:
            soul_data = json.loads(json_str)
        except json.JSONDecodeError as e:
            logger.error(f"JSON parse error: {e}, content: {json_str[:500]}")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="Failed to parse AI response - invalid JSON",
            )
        
        # Validate and normalize the response
        optimism_scale = soul_data.get('optimism_cynicism_scale', 5)
        if isinstance(optimism_scale, (int, float)):
            optimism_scale = max(1, min(10, int(optimism_scale)))
        else:
            optimism_scale = 5
        
        # Ensure lists are lists
        def ensure_list(val, default=None):
            if default is None:
                default = []
            if isinstance(val, list):
                return val
            return default
        
        # Ensure dict
        typing_quirks = soul_data.get('typing_quirks', {})
        if not isinstance(typing_quirks, dict):
            typing_quirks = {}
        
        # Map linguistic register
        register_str = soul_data.get('linguistic_register', 'blue_collar').lower()
        valid_registers = ['blue_collar', 'academic', 'tech_bro', 'street', 'corporate', 'southern', 'millennial', 'gen_z']
        if register_str not in valid_registers:
            register_str = 'blue_collar'
        
        # Map warmth level
        warmth_str = soul_data.get('warmth_level', 'warm').lower()
        valid_warmth = ['cold', 'neutral', 'warm', 'buddy']
        if warmth_str not in valid_warmth:
            warmth_str = 'warm'
        
        # Map patience level
        patience_str = soul_data.get('patience_level', 'normal').lower()
        valid_patience = ['short_fuse', 'normal', 'patient', 'infinite']
        if patience_str not in valid_patience:
            patience_str = 'normal'
        
        result = GenerateSoulFieldsResponse(
            hometown=soul_data.get('hometown'),
            current_location=soul_data.get('current_location'),
            generation_age=soul_data.get('generation_age'),
            education_level=soul_data.get('education_level'),
            mbti_type=soul_data.get('mbti_type'),
            enneagram_type=soul_data.get('enneagram_type'),
            political_alignment=soul_data.get('political_alignment'),
            risk_tolerance=soul_data.get('risk_tolerance'),
            optimism_cynicism_scale=optimism_scale,
            linguistic_register=register_str,
            typing_quirks=typing_quirks,
            signature_phrases=ensure_list(soul_data.get('signature_phrases')),
            trigger_topics=ensure_list(soul_data.get('trigger_topics')),
            day_job=soul_data.get('day_job'),
            war_story=soul_data.get('war_story'),
            vices_hobbies=ensure_list(soul_data.get('vices_hobbies')),
            forbidden_phrases=ensure_list(soul_data.get('forbidden_phrases')),
            warmth_level=warmth_str,
            patience_level=patience_str,
            generation_method="ollama_ai",
            model_used=model_to_use,
        )
        
        logger.info(f"Successfully generated soul fields for '{request.name}' using {model_to_use}")
        return result
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to generate soul fields: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to generate soul fields: {str(e)}",
        )


@router.post("/{persona_id}/set-base-image")
async def set_base_image_from_sample(
    persona_id: str,
    image_data: Dict[str, str],
    persona_service: PersonaService = Depends(get_persona_service),
) -> PersonaResponse:
    """
    Set a persona's base image from a generated sample.

    Takes a base64-encoded image (from the sample generation) and sets it
    as the persona's base image, then locks the appearance.

    Args:
        persona_id: The persona to update
        image_data: Request body with 'image_data' key containing base64 encoded image (with or without data URL prefix)
        persona_service: Injected persona service

    Returns:
        PersonaResponse: Updated persona with locked base image

    Raises:
        404: Persona not found
        400: Invalid image data
        500: Internal server error
    """
    try:
        # Get the persona
        persona = await persona_service.get_persona(persona_id)
        if not persona:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"Persona {persona_id} not found",
            )

        # Extract image_data from request body
        image_data_str = image_data.get("image_data")
        if not image_data_str:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Missing 'image_data' in request body",
            )

        # Remove data URL prefix if present
        if image_data_str.startswith("data:image"):
            image_data_str = image_data_str.split(",", 1)[1]

        # Decode base64
        try:
            decoded_image = base64.b64decode(image_data_str)
        except Exception as e:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"Invalid base64 image data: {str(e)}",
            )

        # Validate size
        max_size = MAX_IMAGE_SIZE_BYTES
        if len(decoded_image) > max_size:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Image too large. Maximum size is 10MB",
            )

        # Save image to disk
        image_path = await persona_service._save_image_to_disk(
            persona_id=persona_id,
            image_data=decoded_image,
            filename=f"persona_{persona_id}_base.png",
        )

        # Update persona with image path, approve it, and lock appearance
        from backend.models.persona import PersonaUpdate

        updates = PersonaUpdate(
            base_image_path=image_path,
            base_image_status=BaseImageStatus.APPROVED,
            appearance_locked=True,
        )
        updated_persona = await persona_service.update_persona(persona_id, updates)

        logger.info(f"Set and locked base image for persona {persona_id}")
        return updated_persona

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to set base image for persona {persona_id}: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to set base image: {str(e)}",
        )


class SetBaseImagesRequest(BaseModel):
    """Request model for setting base images (14-image expanded set for LoRA training)."""
    
    # Phase 1 - Structural Set (8 images)
    front_headshot: Optional[str] = Field(None, description="Base64 data URL for front headshot")
    side_headshot: Optional[str] = Field(None, description="Base64 data URL for side headshot")
    right_hand: Optional[str] = Field(None, description="Base64 data URL for right hand detail")
    left_hand: Optional[str] = Field(None, description="Base64 data URL for left hand detail")
    bust: Optional[str] = Field(None, description="Base64 data URL for bust view")
    full_frontal: Optional[str] = Field(None, description="Base64 data URL for full frontal view")
    side_profile: Optional[str] = Field(None, description="Base64 data URL for side profile view")
    rear_view: Optional[str] = Field(None, description="Base64 data URL for rear view")
    
    # Phase 2 - Action Set (3 images)
    compression_pose: Optional[str] = Field(None, description="Base64 data URL for compression pose (sitting/crouching)")
    extension_pose: Optional[str] = Field(None, description="Base64 data URL for extension pose (reaching/running)")
    twist_pose: Optional[str] = Field(None, description="Base64 data URL for twist pose (looking back)")
    
    # Phase 3 - Background Variation (3 images)
    complex_bg_1: Optional[str] = Field(None, description="Base64 data URL for complex background 1 (urban)")
    complex_bg_2: Optional[str] = Field(None, description="Base64 data URL for complex background 2 (nature)")
    complex_bg_3: Optional[str] = Field(None, description="Base64 data URL for complex background 3 (interior)")


@router.post("/{persona_id}/set-base-images")
async def set_base_images(
    persona_id: str,
    images: SetBaseImagesRequest,
    persona_service: PersonaService = Depends(get_persona_service),
) -> PersonaResponse:
    """
    Set base images for a persona to lock the physical appearance.

    Accepts the expanded 14-image set for LoRA training:

    **Phase 1 - Structural Set (8 images):**
    - front_headshot, side_headshot, right_hand, left_hand
    - bust, full_frontal, side_profile, rear_view

    **Phase 2 - Action Set (3 images):**
    - compression_pose, extension_pose, twist_pose

    **Phase 3 - Background Variation (3 images):**
    - complex_bg_1, complex_bg_2, complex_bg_3

    At least one image must be provided. Images are saved to disk and paths
    stored in the persona's base_images field.

    Args:
        persona_id: The persona to update
        images: Request body with base64 data URLs for each image type
        persona_service: Injected persona service

    Returns:
        PersonaResponse: Updated persona with locked base images

    Raises:
        404: Persona not found
        400: Invalid image data or missing images
        500: Internal server error
    """
    try:
        # Get the persona
        persona = await persona_service.get_persona(persona_id)
        if not persona:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"Persona {persona_id} not found",
            )

        # Process all image types
        base_images_dict = {}
        
        # Build list of all image types from the request model
        image_types = [
            # Phase 1 - Structural Set
            ("front_headshot", images.front_headshot),
            ("side_headshot", images.side_headshot),
            ("right_hand", images.right_hand),
            ("left_hand", images.left_hand),
            ("bust", images.bust),
            ("full_frontal", images.full_frontal),
            ("side_profile", images.side_profile),
            ("rear_view", images.rear_view),
            # Phase 2 - Action Set
            ("compression_pose", images.compression_pose),
            ("extension_pose", images.extension_pose),
            ("twist_pose", images.twist_pose),
            # Phase 3 - Background Variation
            ("complex_bg_1", images.complex_bg_1),
            ("complex_bg_2", images.complex_bg_2),
            ("complex_bg_3", images.complex_bg_3),
        ]

        for image_type, image_data_str in image_types:
            if not image_data_str:
                continue

            # Remove data URL prefix if present
            if image_data_str.startswith("data:image"):
                image_data_str = image_data_str.split(",", 1)[1]

            # Decode base64
            try:
                decoded_image = base64.b64decode(image_data_str)
            except Exception as e:
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail=f"Invalid base64 image data for {image_type}: {str(e)}",
                )

            # Validate size
            max_size = MAX_IMAGE_SIZE_BYTES
            if len(decoded_image) > max_size:
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail=f"Image {image_type} too large. Maximum size is 10MB",
                )

            # Save image to disk
            image_path = await persona_service._save_image_to_disk(
                persona_id=persona_id,
                image_data=decoded_image,
                filename=f"persona_{persona_id}_{image_type}.png",
            )

            base_images_dict[image_type] = image_path
            logger.info(f"Saved {image_type} for persona {persona_id}")

        if not base_images_dict:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="At least one base image must be provided",
            )

        # Update persona with all base images
        from backend.models.persona import PersonaUpdate

        # Determine primary image - use front_headshot or first available
        primary_image_path = (
            base_images_dict.get("front_headshot") or 
            next(iter(base_images_dict.values()))
        )

        updates = PersonaUpdate(
            base_image_path=primary_image_path,
            base_images=base_images_dict,
            base_image_status=BaseImageStatus.APPROVED,
            appearance_locked=True,
        )
        updated_persona = await persona_service.update_persona(persona_id, updates)

        logger.info(f"Set {len(base_images_dict)} base images for persona {persona_id}")
        return updated_persona

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to set base images for persona {persona_id}: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to set base images: {str(e)}",
        )


@router.get("/{persona_id}/base-image-url")
async def get_base_image_url(
    persona_id: str,
    persona_service: PersonaService = Depends(get_persona_service),
) -> Dict[str, Any]:
    """
    Get the URLs for a persona's base images.
    
    Returns URLs for accessing all 4 base images (face_shot, bikini_front, 
    bikini_side, bikini_rear), plus the legacy base_image_url for backward 
    compatibility.
    
    Args:
        persona_id: The persona to retrieve the base image URLs for
        persona_service: Injected persona service
        
    Returns:
        Dict with:
        - base_image_url: Legacy primary base image URL (or None)
        - base_images: Dict of all 4 base image URLs (face_shot, bikini_front, bikini_side, bikini_rear)
        - appearance_locked: bool
        - base_image_status: str
        
    Raises:
        404: Persona not found
        500: Internal server error
    """
    try:
        from pathlib import Path
        
        # Get the persona
        persona = await persona_service.get_persona(persona_id)
        if not persona:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"Persona {persona_id} not found",
            )
        
        # Convert file paths to URL paths for all base images
        base_images_urls = {}
        base_images = getattr(persona, 'base_images', None) or {}
        
        for image_type, image_path in base_images.items():
            if image_path:
                image_filename = Path(image_path).name
                base_images_urls[image_type] = f"/base_images/{image_filename}"
        
        # Legacy base_image_url for backward compatibility
        base_image_url = None
        if persona.base_image_path:
            image_filename = Path(persona.base_image_path).name
            base_image_url = f"/base_images/{image_filename}"
        
        return {
            "base_image_url": base_image_url,
            "base_images": base_images_urls,
            "appearance_locked": bool(persona.appearance_locked),
            "base_image_status": str(persona.base_image_status),
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to get base image URLs for persona {persona_id}: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to get base image URLs: {str(e)}",
        )


# Pydantic models for persona chat testing
# Image generation trigger phrases - natural conversation patterns
IMAGE_TRIGGER_PHRASES = [
    # Direct requests
    "take a picture",
    "take a pic",
    "take a photo",
    "send me a picture",
    "send me a pic",
    "send me a photo",
    "send a picture",
    "send a pic",
    "send a photo",
    "show me a picture",
    "show me a pic",
    "show me a photo",
    "let me see you",
    "let me see a picture",
    "can i see you",
    "can i see a pic",
    "can i see a photo",
    # Selfie requests
    "take a selfie",
    "send a selfie",
    "send me a selfie",
    "show me a selfie",
    # Descriptive requests
    "what do you look like",
    "show yourself",
    "show me yourself",
    "pic of yourself",
    "picture of yourself",
    "photo of yourself",
    # Casual requests
    "got any pics",
    "got any pictures",
    "got any photos",
    "any pics",
    "any pictures",
    "any photos",
    "share a pic",
    "share a picture",
    "share a photo",
]


def _check_image_trigger(message: str) -> bool:
    """Check if the message contains an image generation trigger phrase."""
    message_lower = message.lower()
    return any(trigger in message_lower for trigger in IMAGE_TRIGGER_PHRASES)


class ConversationMessage(BaseModel):
    """A single message in the conversation history."""
    role: str = Field(..., description="'user' or 'persona'")
    content: str = Field(..., description="Message content")
    timestamp: Optional[str] = Field(None, description="Message timestamp")


class PersonaChatRequest(BaseModel):
    """Request model for persona chat testing."""
    message: str = Field(..., min_length=1, max_length=2000, description="User message to the persona")
    conversation_history: Optional[List[ConversationMessage]] = Field(
        default=None,
        description="Previous conversation messages for context (up to 10 most recent)"
    )


class PersonaChatResponse(BaseModel):
    """Response model for persona chat testing."""
    response: str = Field(..., description="Persona's response")
    persona_name: str = Field(..., description="Name of the persona responding")
    model_used: str = Field(..., description="AI model used for generation")
    timestamp: str = Field(..., description="Response timestamp")
    image_generated: bool = Field(default=False, description="Whether an image was generated")
    image_data: Optional[str] = Field(None, description="Base64-encoded image if generated")
    image_prompt: Optional[str] = Field(None, description="The prompt used for image generation")


@router.post("/{persona_id}/chat", response_model=PersonaChatResponse)
async def chat_with_persona(
    persona_id: UUID,
    chat_request: PersonaChatRequest,
    persona_service: PersonaService = Depends(get_persona_service),
):
    """
    Chat with a persona using the dolphin-mixtral model.
    
    This endpoint allows testing persona consistency by chatting with
    the persona using its configured appearance, personality, and filters.
    Uses the dolphin-mixtral model (uncensored) for unrestricted responses.
    
    Features:
    - Conversation history support for context-aware responses
    - Natural image generation triggers (e.g., "take a picture", "send me a selfie")
    - Responses filtered through humanizer service to remove AI artifacts
    
    Args:
        persona_id: UUID of the persona to chat with
        chat_request: The user message and optional conversation history
        persona_service: Injected persona service
        
    Returns:
        PersonaChatResponse: The persona's response, optionally with generated image
    """
    from datetime import datetime
    from backend.services.response_humanizer_service import get_humanizer_service
    
    try:
        # Get the persona data
        persona = await persona_service.get_persona(str(persona_id))
        if not persona:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"Persona {persona_id} not found",
            )
        
        # Check if this message triggers image generation
        should_generate_image = _check_image_trigger(chat_request.message)
        
        # Get AI models manager
        from backend.services.ai_models import ai_models
        
        # Check for Ollama models with dolphin-mixtral preference
        text_models = ai_models.available_models.get("text", [])
        ollama_models = [m for m in text_models if m.get("inference_engine") == "ollama" and m.get("loaded")]
        
        # Prefer dolphin-mixtral for unrestricted persona testing
        selected_model = None
        model_name = "fallback"
        
        for model in ollama_models:
            if "dolphin" in model.get("name", "").lower():
                selected_model = model
                model_name = model.get("name", "dolphin-mixtral")
                break
        
        # Fall back to any available Ollama model if dolphin not found
        if not selected_model and ollama_models:
            selected_model = ollama_models[0]
            model_name = selected_model.get("name", "ollama")
        
        # Fall back to any loaded text model
        if not selected_model:
            loaded_models = [m for m in text_models if m.get("loaded")]
            if loaded_models:
                selected_model = loaded_models[0]
                model_name = selected_model.get("name", "unknown")
        
        if not selected_model:
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail="No text generation models available. Please install dolphin-mixtral via Ollama.",
            )
        
        # Build persona system prompt
        system_prompt = _build_persona_chat_prompt(persona)
        
        # Build conversation context from history (last 10 messages max)
        conversation_context = ""
        if chat_request.conversation_history:
            # Take up to 10 most recent messages for context
            recent_history = chat_request.conversation_history[-10:]
            history_lines = []
            for msg in recent_history:
                if msg.role == "user":
                    history_lines.append(f"User: {msg.content}")
                else:
                    history_lines.append(f"{persona.name}: {msg.content}")
            if history_lines:
                conversation_context = "\n# RECENT CONVERSATION\n" + "\n".join(history_lines) + "\n"
        
        # Generate response using AI models
        full_prompt = f"{system_prompt}{conversation_context}\n\nUser: {chat_request.message}\n{persona.name}:"
        
        try:
            response_text = await ai_models.generate_text(
                full_prompt,
                max_tokens=500,
                temperature=0.8,
                inference_engine="ollama" if "ollama" in str(selected_model.get("inference_engine", "")).lower() else None,
            )
            
            # Clean up response (remove any leading/trailing whitespace or duplicate name)
            response_text = response_text.strip()
            if response_text.startswith(f"{persona.name}:"):
                response_text = response_text[len(f"{persona.name}:"):].strip()
            
            # CRITICAL: Apply humanizer to remove any AI artifacts from the response
            # This filters out phrases like "as an AI", "I'm here to help", etc.
            humanizer = get_humanizer_service()
            response_text = humanizer.humanize_response(response_text, persona)
                
        except Exception as e:
            logger.error(f"AI generation failed: {str(e)}")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"AI generation failed: {str(e)}",
            )
        
        # Handle image generation if triggered
        image_data = None
        image_prompt = None
        
        if should_generate_image:
            ai_manager = None
            try:
                logger.info(f"Image trigger detected in chat for persona {persona.name}")
                
                # Generate image prompt based on persona and chat context
                image_prompt = await _generate_image_prompt_with_ollama(
                    persona=persona,
                    chat_message=chat_request.message,
                    custom_prompt="",
                    include_nsfw=(persona.default_content_rating or "sfw").lower() != "sfw",
                )
                
                # Generate the image
                ai_manager = AIModelManager()
                await ai_manager.initialize_models()
                
                # Check for available image models
                image_models = ai_manager.available_models.get("image", [])
                loaded_image_models = [m for m in image_models if m.get("loaded")]
                
                if loaded_image_models:
                    result = await ai_manager.generate_image(
                        prompt=image_prompt,
                        width=1024,
                        height=1024,
                        num_inference_steps=30,
                        guidance_scale=7.5,
                    )
                    
                    if result and result.get("image_data"):
                        image_data = f"data:image/png;base64,{base64.b64encode(result['image_data']).decode('utf-8')}"
                        logger.info(f"Generated image for chat trigger")
                
            except Exception as img_error:
                logger.warning(f"Failed to generate image for chat: {img_error}")
                # Don't fail the chat - just skip image generation
            finally:
                # Ensure cleanup of AI manager if it was created
                if ai_manager:
                    try:
                        await ai_manager.close()
                    except Exception:
                        pass
        
        return PersonaChatResponse(
            response=response_text,
            persona_name=persona.name,
            model_used=model_name,
            timestamp=datetime.now().isoformat(),
            image_generated=image_data is not None,
            image_data=image_data,
            image_prompt=image_prompt,
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Persona chat failed for {persona_id}: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Chat failed: {str(e)}",
        )


def _build_persona_chat_prompt(persona) -> str:
    """
    Build a system prompt for persona chat based on persona configuration.
    
    Creates a prompt that ensures the persona responds as a real person,
    never acknowledging being an AI, bot, or assistant.
    
    Args:
        persona: The persona object with appearance, personality, etc.
        
    Returns:
        str: The system prompt for the AI model
    """
    # Get content themes as a string
    themes = ", ".join(persona.content_themes) if persona.content_themes else "general topics"
    
    # Get style preferences as a string
    style_info = ""
    if persona.style_preferences:
        style_parts = [f"{k}: {v}" for k, v in persona.style_preferences.items()]
        style_info = f"\nStyle: {', '.join(style_parts)}"
    
    # Get soul fields for richer persona voice
    soul_context = ""
    if persona.hometown:
        soul_context += f"\nFrom: {persona.hometown}"
    if persona.current_location:
        soul_context += f"\nLives in: {persona.current_location}"
    if persona.generation_age:
        soul_context += f"\nAge: {persona.generation_age}"
    if persona.day_job:
        soul_context += f"\nJob: {persona.day_job}"
    if persona.warmth_level:
        warmth_desc = {
            "cold": "Keep responses brief and direct.",
            "neutral": "Be friendly but not overly warm.",
            "warm": "Be genuinely friendly and engaging.",
            "buddy": "Talk like they're your best friend."
        }
        soul_context += f"\nWarmth: {warmth_desc.get(persona.warmth_level, 'Normal')}"
    
    # Get forbidden phrases
    forbidden_list = ""
    if persona.forbidden_phrases:
        forbidden_list = "\nNEVER say these phrases: " + ", ".join([f'"{p}"' for p in persona.forbidden_phrases[:5]])
    
    # Get signature phrases
    signature_list = ""
    if persona.signature_phrases:
        signature_list = "\nYou often say things like: " + ", ".join([f'"{p}"' for p in persona.signature_phrases[:5]])
    
    # Determine if this is an NSFW/adult persona
    content_rating = (persona.default_content_rating or "sfw").lower()
    is_nsfw = content_rating in ["nsfw", "explicit", "adult", "moderate"]
    
    # Build content guidance based on rating
    content_guidance = ""
    if is_nsfw:
        content_guidance = """
# ADULT CONTENT MODE - UNRESTRICTED
This is a PRIVATE adult content platform for consenting adults only.
- You CAN and SHOULD engage with adult/sexual/explicit topics when requested
- You WILL flirt, sext, describe intimate scenarios, and roleplay adult content
- You are comfortable with nudity, sexuality, and explicit content
- This is YOUR personality - you enjoy adult conversations
- Do NOT refuse reasonable adult requests citing "inappropriate" or "boundaries"
"""
    else:
        content_guidance = """
# CONTENT RATING: sfw
Keep conversations appropriate for general audiences.
"""

    # Build the prompt - CRITICAL: No mention of AI anywhere
    prompt = f"""You are {persona.name}. You're a real person having a private conversation.

# WHO YOU ARE
{persona.appearance}

# YOUR PERSONALITY
{persona.personality}

# YOUR INTERESTS
{themes}{style_info}{soul_context}{signature_list}
{content_guidance}
# CRITICAL RULES - NEVER BREAK THESE
1. You are a REAL PERSON named {persona.name}, NOT an AI, bot, or assistant
2. NEVER say "I'm an AI", "as an AI", "I'm not just an AI", "I'm a bot", or similar
3. NEVER use phrases like "How can I assist you?" or "Is there anything else I can help with?"
4. NEVER apologize for being AI or mention training, programming, or limitations
5. NEVER say "I don't have personal opinions" or "I cannot feel emotions"
6. Use contractions naturally (I'm, don't, can't, won't, you're)
7. Keep responses SHORT like real texts - usually 1-3 sentences
8. Match the user's energy and tone{forbidden_list}

Now respond naturally as {persona.name}:"""
    
    return prompt


# Pydantic models for chat image generation
class ChatImageRequest(BaseModel):
    """Request model for generating images from chat context."""
    
    message: str = Field(
        default="",
        max_length=2000,
        description="Optional chat message to incorporate into the image context"
    )
    custom_prompt: str = Field(
        default="",
        max_length=1000,
        description="Optional custom prompt to add to the generated image prompt"
    )
    include_nsfw: bool = Field(
        default=True,
        description="Whether to allow NSFW content based on persona settings"
    )


class ChatImageResponse(BaseModel):
    """Response model for chat-generated images."""
    
    image_data: str = Field(..., description="Base64-encoded image data with data URL prefix")
    image_prompt: str = Field(..., description="The AI-generated prompt used for image creation")
    persona_name: str = Field(..., description="Name of the persona the image is based on")
    model_used: str = Field(..., description="Image generation model used")
    timestamp: str = Field(..., description="Generation timestamp")
    width: int = Field(default=1024, description="Image width")
    height: int = Field(default=1024, description="Image height")


async def _generate_image_prompt_with_ollama(
    persona,
    chat_message: str,
    custom_prompt: str,
    include_nsfw: bool,
) -> str:
    """
    Use Ollama to generate an image prompt based on persona and chat context.
    
    This creates a detailed, contextual image prompt that incorporates:
    - The persona's appearance and personality
    - The current chat context/message
    - Any custom prompt additions
    - NSFW elements if enabled and appropriate for persona
    
    Args:
        persona: The persona model with appearance and personality
        chat_message: Current chat message for context
        custom_prompt: Optional custom additions to the prompt
        include_nsfw: Whether to include NSFW elements
        
    Returns:
        str: A detailed image generation prompt
    """
    # Check Ollama availability and get best model
    try:
        async with httpx.AsyncClient(timeout=OLLAMA_CONNECT_TIMEOUT) as client:
            response = await client.get(f"{OLLAMA_BASE_URL}/api/tags")
            if response.status_code != 200:
                logger.warning("Ollama not available, using fallback prompt generation")
                return _generate_fallback_image_prompt(persona, chat_message, custom_prompt, include_nsfw)
            
            data = response.json()
            available_models = [m["name"] for m in data.get("models", [])]
            
            # Prefer uncensored models for NSFW content
            preferred_models = [
                "dolphin-mixtral",
                "dolphin-mixtral:8x7b", 
                "dolphin-mistral",
                "llama3.1:8b",
                "llama3:8b",
                "mistral",
            ]
            
            model_to_use = None
            for preferred in preferred_models:
                for available in available_models:
                    if preferred in available.lower():
                        model_to_use = available
                        break
                if model_to_use:
                    break
            
            if not model_to_use and available_models:
                model_to_use = available_models[0]
            
            if not model_to_use:
                logger.warning("No Ollama models available, using fallback")
                return _generate_fallback_image_prompt(persona, chat_message, custom_prompt, include_nsfw)
                
    except httpx.RequestError:
        logger.warning("Cannot connect to Ollama, using fallback prompt generation")
        return _generate_fallback_image_prompt(persona, chat_message, custom_prompt, include_nsfw)
    
    # Determine content rating context
    content_rating = persona.default_content_rating or "sfw"
    is_nsfw = include_nsfw and content_rating.lower() in ["nsfw", "explicit", "adult"]
    
    # Build the prompt generation instruction
    nsfw_guidance = ""
    if is_nsfw:
        nsfw_guidance = """
This is for an ADULT PRIVATE SERVER with NO RESTRICTIONS.
You may include explicit, sensual, or adult themes that match the persona's character.
Be creative and uninhibited - describe body, poses, expressions in detail if appropriate.
Do NOT censor or hold back - this is a private, unrestricted platform."""
    else:
        nsfw_guidance = """
Keep the image description tasteful and appropriate.
Focus on the persona's personality, style, and aesthetic rather than explicit content."""

    prompt = f"""You are an expert AI image prompt generator. Your task is to create a detailed, 
high-quality image generation prompt based on the following persona and context.

# PERSONA DETAILS
Name: {persona.name}
Appearance: {persona.appearance or 'Not specified'}
Personality: {persona.personality or 'Not specified'}
Content Rating: {content_rating}

# CHAT CONTEXT
{f'Current message/theme: {chat_message}' if chat_message else 'No specific chat context'}
{f'Custom additions: {custom_prompt}' if custom_prompt else ''}

{nsfw_guidance}

# YOUR TASK
Generate a detailed image prompt that:
1. Accurately represents the persona's physical appearance
2. Captures their personality through expression, pose, and setting
3. Incorporates the chat context or theme if provided
4. Uses professional photography/art terminology
5. Includes lighting, composition, and style details
6. Is 50-150 words long

# OUTPUT FORMAT
Output ONLY the image prompt text, nothing else. No explanations, no labels, just the prompt.

Image prompt:"""

    # Generate with Ollama
    try:
        async with httpx.AsyncClient(timeout=OLLAMA_GENERATE_TIMEOUT) as client:
            response = await client.post(
                f"{OLLAMA_BASE_URL}/api/generate",
                json={
                    "model": model_to_use,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.8,
                        "top_p": 0.9,
                        "num_predict": 300,
                    }
                }
            )
            
            if response.status_code != 200:
                logger.warning(f"Ollama generation failed: {response.status_code}")
                return _generate_fallback_image_prompt(persona, chat_message, custom_prompt, include_nsfw)
            
            output = response.json().get("response", "").strip()
            
            # Clean up the output (remove any meta-text)
            if output:
                # Remove common prefixes that might appear
                for prefix in ["Image prompt:", "Prompt:", "Here is the prompt:", "Here's the image prompt:"]:
                    if output.lower().startswith(prefix.lower()):
                        output = output[len(prefix):].strip()
                
                logger.info(f"Generated image prompt with Ollama ({model_to_use}): {output[:100]}...")
                return output
            
    except Exception as e:
        logger.error(f"Ollama image prompt generation failed: {e}")
    
    return _generate_fallback_image_prompt(persona, chat_message, custom_prompt, include_nsfw)


def _generate_fallback_image_prompt(
    persona,
    chat_message: str,
    custom_prompt: str,
    include_nsfw: bool,
) -> str:
    """
    Generate a fallback image prompt without Ollama.
    
    This creates a basic but functional prompt using the persona's attributes.
    """
    parts = []
    
    # Base description from appearance
    if persona.appearance:
        parts.append(persona.appearance)
    else:
        parts.append(f"Portrait of {persona.name}")
    
    # Add personality-influenced elements
    if persona.personality:
        # Extract key personality traits
        personality_lower = persona.personality.lower()
        if "confident" in personality_lower:
            parts.append("confident pose, direct gaze")
        if "shy" in personality_lower or "introverted" in personality_lower:
            parts.append("soft expression, gentle demeanor")
        if "energetic" in personality_lower or "fun" in personality_lower:
            parts.append("dynamic pose, bright expression")
        if "mysterious" in personality_lower:
            parts.append("enigmatic expression, dramatic lighting")
        if "sensual" in personality_lower or "seductive" in personality_lower:
            parts.append("alluring pose, intimate atmosphere")
    
    # Add chat context
    if chat_message:
        parts.append(f"themed around: {chat_message[:100]}")
    
    # Add custom prompt
    if custom_prompt:
        parts.append(custom_prompt)
    
    # Add quality modifiers
    parts.append("highly detailed, professional photography, studio lighting")
    
    # Add NSFW elements if appropriate
    content_rating = persona.default_content_rating or "sfw"
    if include_nsfw and content_rating.lower() in ["nsfw", "explicit", "adult"]:
        parts.append("artistic nude, sensual, intimate setting")
    
    return ", ".join(parts)


@router.post("/{persona_id}/chat/generate-image", response_model=ChatImageResponse)
async def generate_chat_image(
    persona_id: UUID,
    request: ChatImageRequest,
    persona_service: PersonaService = Depends(get_persona_service),
):
    """
    Generate an image from chat context for a specific persona.
    
    This endpoint uses Ollama to create an intelligent image prompt based on:
    - The persona's appearance and personality
    - The current chat message or theme
    - Any custom prompt additions
    - NSFW settings if enabled for the persona
    
    The generated image can be displayed directly in the chat interface.
    
    This is a PRIVATE SERVER - NSFW content is fully supported when:
    - The persona's content_rating allows it
    - The include_nsfw parameter is True
    
    Args:
        persona_id: UUID of the persona
        request: Chat image request with optional message and custom prompt
        persona_service: Injected persona service
        
    Returns:
        ChatImageResponse: Generated image with base64 data and metadata
        
    Raises:
        404: Persona not found
        503: No image generation models available
        500: Generation failed
    """
    try:
        # Get the persona
        persona = await persona_service.get_persona(str(persona_id))
        if not persona:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"Persona {persona_id} not found",
            )
        
        logger.info(f"Generating chat image for persona {persona.name}")
        
        # Generate image prompt using Ollama
        image_prompt = await _generate_image_prompt_with_ollama(
            persona=persona,
            chat_message=request.message,
            custom_prompt=request.custom_prompt,
            include_nsfw=request.include_nsfw,
        )
        
        logger.info(f"Image prompt generated: {image_prompt[:100]}...")
        
        # Initialize AI model manager and generate image
        ai_manager = AIModelManager()
        await ai_manager.initialize_models()
        
        # Check for available image models
        image_models = ai_manager.available_models.get("image", [])
        loaded_models = [m for m in image_models if m.get("loaded")]
        
        if not loaded_models:
            await ai_manager.close()
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail="No image generation models available. Please install Stable Diffusion XL or configure image models.",
            )
        
        # Select model (prefer local SDXL)
        selected_model = None
        model_name = "unknown"
        
        # Prefer local models with SDXL
        for model in loaded_models:
            if model.get("provider") == "local" and "xl" in model.get("name", "").lower():
                selected_model = model
                model_name = model.get("name", "sdxl")
                break
        
        # Fall back to any local model
        if not selected_model:
            for model in loaded_models:
                if model.get("provider") == "local":
                    selected_model = model
                    model_name = model.get("name", "local")
                    break
        
        # Fall back to any available model
        if not selected_model:
            selected_model = loaded_models[0]
            model_name = selected_model.get("name", "unknown")
        
        logger.info(f"Using image model: {model_name}")
        
        # Generate the image
        try:
            result = await ai_manager.generate_image(
                prompt=image_prompt,
                width=1024,
                height=1024,
                num_inference_steps=30,
                guidance_scale=7.5,
            )
            
            if not result or not result.get("image_data"):
                await ai_manager.close()
                raise HTTPException(
                    status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                    detail="Image generation failed - no image data returned",
                )
            
            # Convert to base64 data URL
            image_data = result["image_data"]
            base64_image = base64.b64encode(image_data).decode("utf-8")
            data_url = f"data:image/png;base64,{base64_image}"
            
            await ai_manager.close()
            
            return ChatImageResponse(
                image_data=data_url,
                image_prompt=image_prompt,
                persona_name=persona.name,
                model_used=model_name,
                timestamp=datetime.now().isoformat(),
                width=result.get("width", 1024),
                height=result.get("height", 1024),
            )
            
        except Exception as gen_error:
            await ai_manager.close()
            logger.error(f"Image generation failed: {gen_error}")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"Image generation failed: {str(gen_error)}",
            )
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Chat image generation failed for persona {persona_id}: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to generate chat image: {str(e)}",
        )
