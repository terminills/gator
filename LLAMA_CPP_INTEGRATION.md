# llama.cpp Integration Guide

## Overview

This document explains how llama.cpp is integrated into the Gator AI Influencer Platform to provide **real text generation** capabilities.

## Problem Statement

Prior to this integration, content generation appeared to succeed but was actually only creating database records with placeholder data. When AI models failed to load or execute, the system would:

1. Return empty `image_data` or placeholder text
2. Write empty files to disk
3. Create database records as if generation succeeded
4. Report "CONTENT GENERATION COMPLETE" even though no AI processing occurred

**The tests only proved the database worked, not that actual AI generation was happening.**

## Solution

We've integrated llama.cpp as a vendored dependency to provide guaranteed text generation capabilities:

### 1. llama.cpp as Third-Party Dependency

- **Location**: `third_party/llama.cpp/`
- **Build**: CMake-based build system
- **Binary**: `third_party/llama.cpp/build/bin/llama-cli`
- **Version**: Latest master branch (shallow clone)

### 2. Build Script

```bash
./scripts/build_llamacpp.sh
```

This script:
- Clones llama.cpp if not present
- Builds the `llama-cli` binary with CPU optimizations
- Outputs build status and binary location

### 3. Integration Test

```bash
python test_llamacpp_integration.py
```

This test **proves** that:
- llama-cli binary exists and runs
- AI models service can detect llama.cpp
- Model directory structure is correct
- System is ready for actual content generation

## Building llama.cpp

### Prerequisites

- CMake >= 3.14
- C++ compiler with C++17 support (g++, clang)
- Make or Ninja

### Build Steps

```bash
cd /path/to/gator
./scripts/build_llamacpp.sh
```

### Manual Build

```bash
cd third_party/llama.cpp
mkdir -p build && cd build
cmake .. -DCMAKE_BUILD_TYPE=Release -DGGML_NATIVE=OFF -DLLAMA_CURL=OFF
make -j$(nproc) llama-cli
```

### Verify Installation

```bash
third_party/llama.cpp/build/bin/llama-cli --version
```

## Model Setup

### Download Models

Models should be placed in `models/text/<model-name>/`:

```bash
# Example: TinyLlama 1.1B (680MB, good for testing)
mkdir -p models/text/tinyllama
cd models/text/tinyllama
curl -L -O https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf
```

### Supported Model Formats

- **GGUF** (.gguf) - Recommended, most efficient
- Quantized models (Q4_K_M, Q5_K_M, Q8_0) for different speed/quality tradeoffs

### Model Recommendations

| Model | Size | RAM Required | Use Case |
|-------|------|--------------|----------|
| TinyLlama 1.1B Q4_K_M | 680MB | 2GB | Testing, development |
| Llama 3.1 8B Q4_K_M | 4.7GB | 8GB | Production, fast |
| Llama 3.1 8B Q8_0 | 8.5GB | 12GB | Production, quality |
| Qwen2.5 7B Q4_K_M | 4.4GB | 8GB | Multilingual |

## AI Models Service Integration

The `ai_models.py` service automatically detects llama.cpp when:

1. `llama-cli` or `main` binary is in PATH
2. Model files exist in `models/text/` directory
3. Inference engine is set to `llama.cpp`

### Configuration

Models using llama.cpp should be configured with:

```python
{
    "name": "tinyllama-1.1b",
    "inference_engine": "llama.cpp",
    "path": "models/text/tinyllama/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf",
    "model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
    # ... other config
}
```

### Text Generation Flow

1. Service checks if llama.cpp is available
2. Validates model file exists
3. Spawns llama-cli subprocess with appropriate parameters
4. Streams output in real-time to logs
5. Returns generated text (not placeholder!)

## Testing

### Unit Test

```bash
python test_llamacpp_integration.py
```

Expected output:
```
‚úÖ All tests passed! llama.cpp integration is working.
```

### Integration Test

Once models are downloaded:

```bash
# Test text generation through API
curl -X POST http://localhost:8000/api/v1/content/generate \
  -H "Content-Type: application/json" \
  -d '{
    "content_type": "text",
    "prompt": "Write a short social media post about AI",
    "quality": "standard"
  }'
```

Check logs for:
```
ü¶ô Starting llama.cpp engine...
RAW LLAMA.CPP ENGINE OUTPUT (LIVE):
[actual model output here]
```

## Troubleshooting

### llama-cli not found

```bash
# Add to PATH temporarily
export PATH="$PWD/third_party/llama.cpp/build/bin:$PATH"

# Or rebuild
./scripts/build_llamacpp.sh
```

### Model file not found

```bash
# Check model directory
ls -lh models/text/*/

# Expected: .gguf files
```

### Build fails

```bash
# Install dependencies (Ubuntu/Debian)
sudo apt-get update
sudo apt-get install -y build-essential cmake

# Rebuild
cd third_party/llama.cpp
rm -rf build
cd ../../
./scripts/build_llamacpp.sh
```

### Generation returns placeholder

If you see `[vLLM generation with ...]` or similar placeholder:
1. Check llama-cli is in PATH: `which llama-cli`
2. Verify model file exists and is readable
3. Check logs for actual error message
4. Ensure sufficient RAM (model size + 2GB overhead)

## Architecture

### Before (Broken)

```
API Request ‚Üí Content Service ‚Üí AI Models Service
                                     ‚Üì
                                 [Try model]
                                     ‚Üì
                                 [Fails silently]
                                     ‚Üì
                                 [Return placeholder]
                                     ‚Üì
Content Service ‚Üê ‚Üê ‚Üê ‚Üê ‚Üê ‚Üê ‚Üê ‚Üê ‚Üê ‚Üê ‚Üê
     ‚Üì
[Write empty file]
     ‚Üì
[Create DB record]
     ‚Üì
[Report "SUCCESS"]  ‚ùå WRONG!
```

### After (Fixed)

```
API Request ‚Üí Content Service ‚Üí AI Models Service
                                     ‚Üì
                                 [Detect llama.cpp]
                                     ‚Üì
                                 [Spawn llama-cli]
                                     ‚Üì
                                 [Stream real output]
                                     ‚Üì
Content Service ‚Üê ‚Üê ‚Üê ‚Üê ‚Üê ‚Üê ‚Üê ‚Üê ‚Üê ‚Üê ‚Üê
     ‚Üì
[Write actual content]
     ‚Üì
[Create DB record]
     ‚Üì
[Report "SUCCESS"]  ‚úÖ CORRECT!
```

## Performance

### CPU Inference

- **TinyLlama 1.1B Q4_K_M**: ~10-20 tokens/second (4 cores)
- **Llama 3.1 8B Q4_K_M**: ~3-8 tokens/second (4 cores)

### GPU Acceleration (Optional)

llama.cpp supports GPU acceleration:

```bash
# CUDA
cmake .. -DGGML_CUDA=ON

# ROCm (AMD)
cmake .. -DGGML_HIP=ON

# Metal (macOS)
cmake .. -DGGML_METAL=ON
```

Rebuild llama.cpp with appropriate flags for your hardware.

## Roadmap

- [x] Integrate llama.cpp for text generation
- [x] Create build scripts and documentation
- [x] Add integration tests
- [ ] Update AI models service to prefer llama.cpp
- [ ] Remove placeholder fallback behavior
- [ ] Add proper error handling (fail-fast, not silent)
- [ ] Create end-to-end test with real model
- [ ] Add GPU acceleration support
- [ ] Performance benchmarking

## References

- llama.cpp GitHub: https://github.com/ggerganov/llama.cpp
- GGUF format spec: https://github.com/ggerganov/ggml/blob/master/docs/gguf.md
- Model downloads: https://huggingface.co/models?library=gguf
- Quantization guide: https://github.com/ggerganov/llama.cpp/blob/master/examples/quantize/README.md
